{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx06mo_SI5sT"
      },
      "source": [
        "# Path to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duh7fkJVIFMJ",
        "outputId": "3d911ed2-26a1-46fb-a4f0-a57e4c8648aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_training_data/multi-modality_images/'#path to dataset\n",
        "os.chdir(path_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFI8Ds2VJMvB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGTsyeK7a6nT",
        "outputId": "c580f3b4-dc6f-4fab-c55d-c79b186e9f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/612.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/612.1 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkg1_ngQTDcx",
        "outputId": "8c3230f0-6408-4bed-edd7-cd727e4a3b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.10.0 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwqbK7PSUDG"
      },
      "source": [
        "# 3D CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mylq9tzuSX4O",
        "outputId": "bd2defa2-b825-48b4-b3ba-be81529c7418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting classification-models-3D\n",
            "  Downloading classification_models_3D-1.0.7-py3-none-any.whl (69 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/69.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/69.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m61.4/69.5 kB\u001b[0m \u001b[31m976.8 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: classification-models-3D\n",
            "Successfully installed classification-models-3D-1.0.7\n",
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m395.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_applications) (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_applications) (3.8.0)\n",
            "Installing collected packages: keras_applications\n",
            "Successfully installed keras_applications-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install classification-models-3D\n",
        "!pip install keras_applications\n",
        "from classification_models_3D.tfkeras import Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3A4VA8dv05n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from collections import Counter#\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aGk1GntJPWa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from io import BytesIO\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, Concatenate, Reshape, GlobalMaxPooling2D, GlobalMaxPooling3D, GlobalAveragePooling3D, Conv2D, Conv1D, Add\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score, roc_curve,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import random\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from tensorflow.keras import layers\n",
        "import csv\n",
        "from utils_GAMMA_V2 import padroniza_resultado\n",
        "from utils_GAMMA_V2 import converte\n",
        "from scipy import stats\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Sequential, Model\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from keras.backend import int_shape\n",
        "import pickle\n",
        "import optuna\n",
        "from keras.applications.densenet import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_LCC90hOA38"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Dzcr6_OEWH"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224 #\n",
        "depth = 64\n",
        "# TRAINING\n",
        "EPOCHS = 150\n",
        "early_stop_epochs = 2\n",
        "learning_rate_epochs = 5\n",
        "optimizer_direction = ['minimize', \"maximize\"]\n",
        "#Foldes to save models and dataframes results\n",
        "teste_number = '1/'\n",
        "dir_save = '/content/Test_CNN/Models/Test_' + teste_number\n",
        "results_dir = '/content/Test_CNN/Results_optuna/Test_' + teste_number\n",
        "classification_results_dir = '/content/Test_CNN/Classification Results Optuna/Test_' + teste_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nChTJ8nZIeaE"
      },
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qszb6AnZIeu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f60e9f0-812a-4840-d009-c9a91aed5d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "folder_fundus = 'dataset_fundus/train/fundus_images flip - 100 images/'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "\n",
        "conj = 0# 0--> load train images; 1--> load val images\n",
        "X_train_fundus,X_train_ROI,Y = load_fundus_images(path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)\n",
        "num_classes = len(np.unique(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sH0qrRfCK2H"
      },
      "source": [
        "Read OCTs Volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl69maz-CKJB"
      },
      "outputs": [],
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "scan_paths = dir_octs(path_dataset,'multi-modality_images',0)#path to 3D volumes\n",
        "X_oct = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read 3D volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfWV4riR5cOX"
      },
      "source": [
        "Split Train/Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundus"
      ],
      "metadata": {
        "id": "bHzmMo2Pwner"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJaA-9Q65feN",
        "outputId": "f73ed4f8-7f7f-409b-d99a-780d0a8c073f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_fundus, x_val_fundus, y_train, y_val = train_test_split(X_train_fundus, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_fundus.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_fundus.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdt6hjh47CB2"
      },
      "source": [
        "ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRcvKYwe6t8o",
        "outputId": "6b41e826-b0f7-451b-f75c-abd0bb1e2d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_ROI, x_val_ROI, y_train, y_val = train_test_split(X_train_ROI, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_ROI.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_ROI.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXteIGLv7ExN"
      },
      "source": [
        "OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqdcH2sue_mY"
      },
      "outputs": [],
      "source": [
        "x_train_oct, x_val_oct, y_train, y_val = train_test_split(X_oct, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_oct.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_oct.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Test"
      ],
      "metadata": {
        "id": "70gIQIWCyuC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-h-gFAFTTov",
        "outputId": "2f7de93e-3ea0-4129-bfbc-ed52f672cf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "x_teste shape: (100, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "folder_fundus = 'fundus_images'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "conj = 1# 1-->Load Images from test set\n",
        "x_test_fundus,x_test_ROI = load_fundus_images(val_path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)#\n",
        "print(f\"x_teste shape: {x_test_fundus.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read OCT Volumes"
      ],
      "metadata": {
        "id": "boD-pbzx0mKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "conj = 1#conj = 0 (Train); conj = 1(Test)\n",
        "scan_paths = dir_octs(val_path_dataset,'multi-modality_images',conj)\n",
        "#scan_paths = scan_paths[:1]\n",
        "X_test_OCT = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read volumes"
      ],
      "metadata": {
        "id": "JP5QL2Kg0lcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVcEwSxPWRh"
      },
      "source": [
        "# Plot Fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hmDQk3XPYRy"
      },
      "outputs": [],
      "source": [
        " # number of images\n",
        "n_images = 100\n",
        "\n",
        "# arrays\n",
        "x = X_train_fundus\n",
        "y = Y\n",
        "\n",
        "# numer of rows and columns\n",
        "rows = 10\n",
        "cols = 10\n",
        "\n",
        "# empty figure\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(28, 28))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# para cada imagem\n",
        "for i in range(n_images):\n",
        "    #subplot\n",
        "    axs[i].imshow(x[i], cmap='gray')\n",
        "    axs[i].set_title(f'Label: {y[i]}')\n",
        "    # remove as bordas e o eixo x, y\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate CSV to be evaluated online"
      ],
      "metadata": {
        "id": "4A64nNEz2F3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gerar o CSV com os resultados obtidos no slit de validação\n",
        "def gera_csv(predicted_classes, classification_results_dir, trial_number):\n",
        "  #classification_results_dir = 'content/Test_CNN/Classification Results Optuna/Test_'+teste_number\n",
        "  if not Path(classification_results_dir).is_dir():\n",
        "    os.mkdir(classification_results_dir)\n",
        "  header = ['data','non','early','mid_advanced']\n",
        "  results = padroniza_resultado(predicted_classes)\n",
        "  #Alterar a pasta\n",
        "  results_file_path = os.path.join(val_path_dataset, classification_results_dir  + '/Classification_Results_'+trial_number+'.csv')\n",
        "  with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)"
      ],
      "metadata": {
        "id": "CFQTdFCY2G-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKUezmqObFkP"
      },
      "source": [
        "# Optimize 3Level Models - V1\n",
        "Load 1Level Models without classifier.\n",
        "Feature extractor layers are frozen\n",
        "FC conected layers are added and trained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88L5wCFkbLkr"
      },
      "outputs": [],
      "source": [
        "def objective(trial):#\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  #freeze_layers = trial.suggest_categorical('freeze_layers',[10, 15, 20, 23])\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
        "\n",
        "  #best model fed with fundus images\n",
        "  best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_1/22_cnn.h5')#VGG19\n",
        "\n",
        "  #best model fed with roi images\n",
        "  best_model_ROI = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_2A/16_cnn.h5')#VGG19\n",
        "\n",
        "  #best model fed with 3d oct\n",
        "  best_model_OCT = keras.models.load_model('Modelos_CNN_Optuna/Testes_30_Trials/Teste_3/11_cnn.h5')#Dense121 - 3D\n",
        "\n",
        "  best_model_fundus._name = \"Level_1\"\n",
        "  best_model_ROI._name = \"Level_2\"\n",
        "  best_model_OCT._name = \"Level_3\"\n",
        "\n",
        "  for layer in best_model_ROI.layers:\n",
        "    layer._name = layer.name + str(\"_2\")\n",
        "\n",
        "  for layer in best_model_OCT.layers:\n",
        "    layer._name = layer.name + str(\"_3\")\n",
        "\n",
        "  best_model_fundus.trainable = False #All feature extractor Layers are frozen\n",
        "  best_model_ROI.trainable = False #All feature extractor Layers are frozen\n",
        "  best_model_OCT.trainable = False #All feature extractor Layers are frozen\n",
        "\n",
        "  Level_1 = best_model_fundus.layers[-6].output\n",
        "  Level_2 = best_model_ROI.layers[-6].output\n",
        "  Level_3 = best_model_OCT.layers[-7].output\n",
        "\n",
        "  Level_1 = Reshape(target_shape=((1,) + int_shape(Level_1)[1:]))(Level_1)#expand dimensions of 2 level models\n",
        "  Level_2 = Reshape(target_shape=((1,) + int_shape(Level_2)[1:]))(Level_2)#expand dimensions of 2 level models\n",
        "  Level_3 = Conv2D(512, (1,1), activation='relu')(Level_3)\n",
        "\n",
        "  network = Concatenate(axis=1)([Level_1, Level_2, Level_3])\n",
        "  #network = Add()([Level_1, Level_2, Level_3])\n",
        "  network = Flatten()(network)\n",
        "  network = Dense(num_dense_nodes, activation='relu')(network)\n",
        "  network = Dropout(drop_out_rate)(network)\n",
        "  network = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(network)\n",
        "  network = Dropout(drop_out_rate)(network)\n",
        "  predictions = Dense(num_classes, activation='softmax')(network)\n",
        "\n",
        "  model = Model(inputs = [best_model_fundus.input, best_model_ROI.input, best_model_OCT.input], outputs = predictions)\n",
        "\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',])#loss='sparse_categorical_crossentropy'\n",
        "  #for i, w in enumerate(model.weights): print(i, w.name)\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  fn = dir_save + str(trial.number) + '_cnn.h5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, verbose=1, mode='auto'),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=0, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss',verbose=1,save_best_only=True)\n",
        "                    ]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=[x_train_fundus, x_train_ROI, x_train_oct], y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                           validation_data = ([x_val_fundus, x_val_ROI, x_val_oct],y_val),\n",
        "                          #validation_split=0.1,\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjM_QudD7nP9"
      },
      "source": [
        "# Optimize 3 Level Models - V2\n",
        "Load model with Imagenet Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pcL-FErM73Ss"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3, 4, ])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['InceptionV3','VGG19','resnet50', 'resnet101', 'resnet152', 'Densenet121', 'Densenet169'])\n",
        "  #CNNs: Dense121, Dense 169, ResNet50, ResNet101, Resnet152, VGG19, Inception V3\n",
        "  canais = 3\n",
        "  if cnn_model == 'VGG19':\n",
        "    model_fundus = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    Rede3D, preprocess_input = Classifiers.get('vgg19')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    model_fundus = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    model_fundus = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    model_fundus = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    model_fundus = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    Rede3D, preprocess_input = Classifiers.get('densenet121')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    model_fundus = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    Rede3D, preprocess_input = Classifiers.get('densenet169')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  model_fundus._name = \"Level_1\"\n",
        "  model_ROI._name = \"Level_2\"\n",
        "  model_oct._name = \"Level_3\"\n",
        "\n",
        "  for layer in model_ROI.layers:\n",
        "      layer._name = layer.name + str(\"_2\")\n",
        "\n",
        "  for layer in model_oct.layers:\n",
        "    layer._name = layer.name + str(\"_3\")\n",
        "\n",
        "  Level_1 = model_fundus.output\n",
        "  Level_2 = model_ROI.output\n",
        "  Level_3 = model_oct.output\n",
        "\n",
        "  Level_1 = Reshape(target_shape=((1,) + int_shape(Level_1)[1:]))(Level_1)#expand dimensions\n",
        "  Level_2 = Reshape(target_shape=((1,) + int_shape(Level_2)[1:]))(Level_2)#expand dimensions\n",
        "\n",
        "  x = Concatenate(axis=1)([Level_1, Level_2, Level_3])\n",
        "  #x = Add()([Level_1, Level_2, Level_3])\n",
        "  x = Flatten()(x)\n",
        "\n",
        "  #MLP\n",
        "  #x = Dense(num_dense_nodes, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  if num_layers == 2:\n",
        "    #x = Dense(num_dense_nodes//dense_nodes_divisor, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "    x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "    x = Dropout(drop_out_rate)(x)\n",
        "\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instanciar e compilar modelo\n",
        "  model = Model(inputs = [model_fundus.input,model_ROI.input, model_oct.input], outputs = output_tensor)\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  #fn = dir_save + str(trial.number) + '_cnn.h5'\n",
        "  fn = dir_save + str(trial.number) + '_cnn.hdf5'#somente pesos\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, verbose=1, mode='auto'),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=0, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss', verbose=1,save_weights_only=True, save_best_only=True)]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=[x_train_fundus, x_train_ROI, x_train_oct], y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = ([x_val_fundus, x_val_ROI, x_val_oct], y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-Ec_iVizQA"
      },
      "source": [
        "#Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zerlCBMWgNn"
      },
      "outputs": [],
      "source": [
        "val_path_dataset = '/content/Test_CNN/'\n",
        "os.chdir(val_path_dataset)\n",
        "\n",
        "if not Path(dir_save).is_dir():\n",
        "  os.mkdir(dir_save)\n",
        "\n",
        "print('\\n*** starting at',pd.Timestamp.now())\n",
        "start_time_total = time.time()\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(directions=optimizer_direction,study_name=\"starter-experiment\")\n",
        "study.optimize(objective, n_trials=5,gc_after_trial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbX03qiOGgV",
        "outputId": "85af87f1-ed66-45e9-fd63-f4b4954d8197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "total elapsed time = 9.191481061776479  minutes\n"
          ]
        }
      ],
      "source": [
        "# save results\n",
        "df_results = study.trials_dataframe()\n",
        "if not Path(results_dir).is_dir():\n",
        "  os.mkdir(results_dir)\n",
        "df_results.to_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "df_results.to_csv(results_dir + 'df_optuna_results.csv')\n",
        "elapsed_time_total = (time.time()-start_time_total)/60\n",
        "print('\\n\\ntotal elapsed time =',elapsed_time_total,' minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN_1Z9uNjmk"
      },
      "source": [
        "# Sort results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO7C-8OZNivN",
        "outputId": "9b27a060-6f88-41cc-c174-d8cb49dcf94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted CSV file (according to multiple columns) =     Unnamed: 0  number  values_0  values_1              datetime_start  \\\n",
            "2           2       2  0.924700       0.7  2023-07-30 00:40:38.980133   \n",
            "3           3       3  1.032561       0.5  2023-07-30 00:43:31.413669   \n",
            "4           4       4  1.039189       0.5  2023-07-30 00:44:01.420744   \n",
            "1           1       1  1.049983       0.5  2023-07-30 00:39:52.849290   \n",
            "0           0       0  3.275835       0.5  2023-07-30 00:38:13.830793   \n",
            "\n",
            "            datetime_complete                duration  params_batch_size  \\\n",
            "2  2023-07-30 00:43:30.218678  0 days 00:02:51.238545                  1   \n",
            "3  2023-07-30 00:44:01.053716  0 days 00:00:29.640047                  2   \n",
            "4  2023-07-30 00:45:18.021475  0 days 00:01:16.600731                  2   \n",
            "1  2023-07-30 00:40:24.659844  0 days 00:00:31.810554                  1   \n",
            "0  2023-07-30 00:39:26.512972  0 days 00:01:12.682179                  3   \n",
            "\n",
            "  params_cnn_model  params_dense_nodes_divisor  params_drop_out_rate  \\\n",
            "2      Densenet169                           2                   0.0   \n",
            "3            VGG19                           8                   0.0   \n",
            "4            VGG19                           4                   0.4   \n",
            "1            VGG19                           8                   0.2   \n",
            "0         resnet50                           4                   0.0   \n",
            "\n",
            "   params_lr  params_num_dense_nodes  params_num_layers  \\\n",
            "2     0.0001                     128                  1   \n",
            "3     0.0001                      64                  1   \n",
            "4     0.0010                     256                  1   \n",
            "1     0.0001                     128                  1   \n",
            "0     0.0001                     256                  2   \n",
            "\n",
            "   system_attrs_nsga2:generation     state  \n",
            "2                              0  COMPLETE  \n",
            "3                              0  COMPLETE  \n",
            "4                              0  COMPLETE  \n",
            "1                              0  COMPLETE  \n",
            "0                              0  COMPLETE  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "val_path_dataset = '/content/Test_CNN/Results_optuna/Test_'+teste_number#path where optuna dataframe was saved\n",
        "os.chdir(val_path_dataset)\n",
        "dataframe = pd.read_csv('df_optuna_results.csv')\n",
        "dataframe.sort_values(\"values_0\",axis=0,ascending=True, inplace=True, na_position='first')\n",
        "print(\"Sorted CSV file (according to multiple columns) = \", dataframe.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnDo77oT1MI"
      },
      "source": [
        "# Evaluate best models in split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FR0xpKTW1M0"
      },
      "source": [
        "Test with fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT9tHTTKQRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ca25ea-acad-43ee-a185-5104f3b69546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 101ms/step\n"
          ]
        }
      ],
      "source": [
        "best_model = keras.models.load_model('/content/Test_CNN/Models/Test_'+teste_number+'0_cnn.h5')\n",
        "prob_class = best_model.predict(x_test_fundus)#probabilities\n",
        "predicted_classes = np.argmax(prob_class, axis=1)\n",
        "trial_number = 1\n",
        "gera_csv(predicted_classes, classification_results_dir, str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA0tFXlBC2hF"
      },
      "source": [
        "# Ensemble of best models from optimization process"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_final_predictions(xprod,\n",
        "                           models_directory,\n",
        "                           df_params,\n",
        "                           threshold, num_models_accept,\n",
        "                           optimization_direction):\n",
        "\n",
        "    if optimization_direction == 'maximize':\n",
        "        df_params.sort_values(by='values_1', ascending=False, inplace=True)\n",
        "    else:\n",
        "        df_params.sort_values(by='values_0', ascending=True, inplace=True)\n",
        "\n",
        "    # apply threshold\n",
        "    accepted_models_num = 0\n",
        "    list_predicted_prob = []\n",
        "    num_models_total = df_params.shape[0]\n",
        "    for i in range(num_models_total):\n",
        "        if optimization_direction == 'maximize':\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_1'] > threshold\n",
        "        else:\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_0'] < threshold\n",
        "\n",
        "        bool2 = df_params.loc[df_params.index[i],'state'] == 'COMPLETE'\n",
        "        bool3 = accepted_models_num < num_models_accept\n",
        "        if bool1 and bool2 and bool3:\n",
        "            model_number = str(df_params.loc[df_params.index[i],'number'])\n",
        "            #if model_number != '6':\n",
        "            print (model_number)\n",
        "            try:\n",
        "                cnn_model = keras.models.load_model(models_directory + model_number + '_cnn.h5')\n",
        "            except:\n",
        "                print('\\ncould not read model number:',model_number)\n",
        "            else:\n",
        "                list_predicted_prob.append(cnn_model.predict(xprod))\n",
        "                accepted_models_num = accepted_models_num + 1\n",
        "\n",
        "    # compute mean probabilities\n",
        "    mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "    # compute predicted class\n",
        "    # argmax uses 1st ocurrance in case of a tie\n",
        "    y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "    return y_predicted_class\n",
        "\n",
        "# fixed parameters - production (Ensemble)\n",
        "threshold_error =  3# validation loss\n",
        "number_of_models = 5\n",
        "\n",
        "# get optuna results parameters\n",
        "#models_dir = results_directory_stub + 'calibration/'\n",
        "df_parameters = pd.read_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "\n",
        "#results_directory = results_directory_stub + calculation_type + '/'\n",
        "#if not Path(results_directory).is_dir():\n",
        "    #os.mkdir(results_directory)\n",
        "\n",
        "final_result = make_final_predictions(x_test_fundus,\n",
        "                               dir_save,\n",
        "                               df_parameters,\n",
        "                               threshold_error,\n",
        "                               number_of_models, optimizer_direction)\n",
        "\n",
        "trial_number = 100\n",
        "gera_csv(final_result, classification_results_dir, str(trial_number))"
      ],
      "metadata": {
        "id": "D10tSpQUY3-A"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4odujajzijHe",
        "qPN2YsxZmki0",
        "dP4xlwCWNFfn",
        "wKUezmqObFkP",
        "OjM_QudD7nP9",
        "uA0tFXlBC2hF",
        "QFkaOH2XWw69",
        "W4R9n8EEXRcq",
        "sC_a8OAJu7We",
        "9fmQ-6Rv2qTD",
        "C0HE_wEfLkWD",
        "gmuOfFOAf03E",
        "9PoS0_oRjGRZ",
        "BK4BQEDOMIyk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}