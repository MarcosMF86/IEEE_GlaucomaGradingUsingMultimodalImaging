{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx06mo_SI5sT"
      },
      "source": [
        "# Path to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duh7fkJVIFMJ",
        "outputId": "3d911ed2-26a1-46fb-a4f0-a57e4c8648aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_training_data/multi-modality_images/'#path to dataset\n",
        "os.chdir(path_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFI8Ds2VJMvB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGTsyeK7a6nT"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkg1_ngQTDcx"
      },
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwqbK7PSUDG"
      },
      "source": [
        "# 3D CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mylq9tzuSX4O"
      },
      "outputs": [],
      "source": [
        "!pip install classification-models-3D\n",
        "!pip install keras_applications\n",
        "from classification_models_3D.tfkeras import Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3A4VA8dv05n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from collections import Counter#usar para contar a quantidade de uma classe ex: counter(y_train)\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aGk1GntJPWa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from io import BytesIO\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, Concatenate, Reshape, GlobalMaxPooling2D, GlobalMaxPooling3D, GlobalAveragePooling3D, Conv2D, Conv1D, Add\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score, roc_curve,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import random\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from tensorflow.keras import layers\n",
        "import csv\n",
        "from utils_GAMMA_V2 import padroniza_resultado\n",
        "from utils_GAMMA_V2 import converte\n",
        "from scipy import stats\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Sequential, Model\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from keras.backend import int_shape\n",
        "import pickle\n",
        "import optuna\n",
        "from keras.applications.densenet import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_LCC90hOA38"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Dzcr6_OEWH"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224 #---> Definido no arquivo importado vit.py\n",
        "depth = 64\n",
        "# TRAINING\n",
        "EPOCHS = 150\n",
        "early_stop_epochs = 2\n",
        "learning_rate_epochs = 5\n",
        "optimizer_direction = ['minimize', \"maximize\"]\n",
        "number_of_random_points = 25  # random searches to start opt process\n",
        "#Foldes to save models and dataframes results\n",
        "teste_number = '1/'\n",
        "dir_save = '/content/Test_CNN/Models/Test_' + teste_number\n",
        "results_dir = '/content/Test_CNN/Results_optuna/Test_' + teste_number\n",
        "classification_results_dir = '/content/Test_CNN/Classification Results Optuna/Test_' + teste_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nChTJ8nZIeaE"
      },
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qszb6AnZIeu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f60e9f0-812a-4840-d009-c9a91aed5d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "folder_fundus = 'dataset_fundus/train/fundus_images flip - 100 images/'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "\n",
        "conj = 0# 0--> load train images; 1--> load val images\n",
        "X_train_fundus,X_train_ROI,Y = load_fundus_images(path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)\n",
        "num_classes = len(np.unique(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sH0qrRfCK2H"
      },
      "source": [
        "Read OCTs Volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl69maz-CKJB"
      },
      "outputs": [],
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "scan_paths = dir_octs(path_dataset,'multi-modality_images',0)#path to 3D volumes\n",
        "X_oct = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read 3D volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfWV4riR5cOX"
      },
      "source": [
        "Split Train/Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundus"
      ],
      "metadata": {
        "id": "bHzmMo2Pwner"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJaA-9Q65feN",
        "outputId": "f73ed4f8-7f7f-409b-d99a-780d0a8c073f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_fundus, x_val_fundus, y_train, y_val = train_test_split(X_train_fundus, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_fundus.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_fundus.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdt6hjh47CB2"
      },
      "source": [
        "ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRcvKYwe6t8o",
        "outputId": "6b41e826-b0f7-451b-f75c-abd0bb1e2d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_ROI, x_val_ROI, y_train, y_val = train_test_split(X_train_ROI, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_ROI.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_ROI.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXteIGLv7ExN"
      },
      "source": [
        "OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqdcH2sue_mY"
      },
      "outputs": [],
      "source": [
        "x_train_oct, x_val_oct, y_train, y_val = train_test_split(X_oct, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_oct.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_oct.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Test"
      ],
      "metadata": {
        "id": "70gIQIWCyuC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-h-gFAFTTov",
        "outputId": "2f7de93e-3ea0-4129-bfbc-ed52f672cf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "x_teste shape: (100, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "folder_fundus = 'fundus_images'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "conj = 1# 1-->Load Images from test set\n",
        "x_test_fundus,x_test_ROI = load_fundus_images(val_path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)#\n",
        "print(f\"x_teste shape: {x_test_fundus.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read OCT Volumes"
      ],
      "metadata": {
        "id": "boD-pbzx0mKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "conj = 1#conj = 0 (Train); conj = 1(Test)\n",
        "scan_paths = dir_octs(val_path_dataset,'multi-modality_images',conj)\n",
        "#scan_paths = scan_paths[:1]\n",
        "X_test_OCT = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read volumes"
      ],
      "metadata": {
        "id": "JP5QL2Kg0lcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVcEwSxPWRh"
      },
      "source": [
        "# Plot Fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hmDQk3XPYRy"
      },
      "outputs": [],
      "source": [
        " # number of images\n",
        "n_images = 100\n",
        "\n",
        "# arrays\n",
        "x = X_train_fundus\n",
        "y = Y\n",
        "\n",
        "# numer of rows and columns\n",
        "rows = 10\n",
        "cols = 10\n",
        "\n",
        "# empty figure\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(28, 28))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# para cada imagem\n",
        "for i in range(n_images):\n",
        "    #subplot\n",
        "    axs[i].imshow(x[i], cmap='gray')\n",
        "    axs[i].set_title(f'Label: {y[i]}')\n",
        "    # remove as bordas e o eixo x, y\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate CSV to be evaluated online"
      ],
      "metadata": {
        "id": "4A64nNEz2F3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gerar o CSV com os resultados obtidos no slit de validação\n",
        "def gera_csv(predicted_classes, classification_results_dir, trial_number):\n",
        "  #classification_results_dir = 'content/Test_CNN/Classification Results Optuna/Test_'+teste_number\n",
        "  if not Path(classification_results_dir).is_dir():\n",
        "    os.mkdir(classification_results_dir)\n",
        "  header = ['data','non','early','mid_advanced']\n",
        "  results = padroniza_resultado(predicted_classes)\n",
        "  #Alterar a pasta\n",
        "  results_file_path = os.path.join(val_path_dataset, classification_results_dir  + '/Classification_Results_'+trial_number+'.csv')\n",
        "  with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)"
      ],
      "metadata": {
        "id": "CFQTdFCY2G-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP4xlwCWNFfn"
      },
      "source": [
        "# Optimize 2Level Models - V2\n",
        "Load pre-trained one level models without classifiers.\n",
        "Freeze some convolutional blocks\n",
        "New FC Layers are trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDJikRhZNBSY"
      },
      "outputs": [],
      "source": [
        "def objective(trial):#cria e avalia modelo\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  freeze_layers = trial.suggest_categorical('freeze_layers',[10, 15, 20, 23])#depends on the number of layers in the model\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001,])\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])\n",
        "  #cnn_model = trial.suggest_categorical('cnn_model', ['InceptionV3','VGG19','resnet50', 'resnet101', 'resnet152', 'Densenet121', 'Densenet169'])\n",
        "  #CNNs: Dense121, Dense 169, ResNet50, ResNet101, Resnet152, VGG19, Inception V3\n",
        "\n",
        "  #best model using fundus images as input\n",
        "  best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_1/22_cnn.h5')#resolution 128x128\n",
        "  #best model using ROI as input\n",
        "  best_model_ROI = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_2A/16_cnn.h5')#Resolution 128x128\n",
        "\n",
        "  for layer in best_model_ROI.layers:\n",
        "    layer._name = layer.name + str(\"_2\")\n",
        "\n",
        "  best_model_fundus._name = \"Level_1\"\n",
        "  best_model_ROI._name = \"Level_2\"\n",
        "\n",
        "  #best_model_fundus.trainable = False #Weights will not be updated\n",
        "  #best_model_ROI.trainable = False #Weights will not be updated\n",
        "\n",
        "  #freeze convolutions blocks\n",
        "  for layer in best_model_fundus.layers[:-freeze_layers]:\n",
        "    layer.trainable= False\n",
        "\n",
        "  #for layer in best_model_fundus.layers:\n",
        "    #print(layer.trainable)\n",
        "\n",
        "  for layer in best_model_ROI.layers[:-freeze_layers]:\n",
        "    layer.trainable= False\n",
        "\n",
        "  Level_1 = best_model_fundus.layers[-6].output#Eliminate MLP\n",
        "  Level_2 = best_model_ROI.layers[-6].output#Eliminate MLP\n",
        "  #Level_1 = Flatten()(Level_1)\n",
        "  #Level_2 = Flatten()(Level_2)\n",
        "  network = Concatenate(axis=-1)([Level_1, Level_2])\n",
        "  #network = Add()([Level_1, Level_2])\n",
        "  network = Flatten()(network)\n",
        "  network = Dense(num_dense_nodes, activation='relu')(network)\n",
        "  network = Dropout(drop_out_rate)(network)\n",
        "  if num_layers == 2:\n",
        "    network = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(network)\n",
        "    network = Dropout(drop_out_rate)(network)\n",
        "  predictions = Dense(num_classes, activation='softmax')(network)\n",
        "  model = Model(inputs = [best_model_fundus.input, best_model_ROI.input], outputs = predictions)\n",
        "\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',])#loss='sparse_categorical_crossentropy'\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  fn = dir_save + str(trial.number) + '_cnn.hdf5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, verbose=1, mode='auto'),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=0, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss',verbose=1,save_weights_only=True,save_best_only=True)\n",
        "                    ]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=[x_train_fundus, x_train_ROI], y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = ([x_val_fundus, x_val_ROI],y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPN2YsxZmki0"
      },
      "source": [
        "# Optimize 2Level Models -V1\n",
        "Models Load with Imagenet Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmRAzz7ahk2k"
      },
      "outputs": [],
      "source": [
        "def objective(trial):#cria e avalia modelo\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['InceptionV3','VGG19','resnet50', 'resnet101', 'resnet152', 'Densenet121', 'Densenet169'])\n",
        "  #CNNs: Dense121, Dense 169, ResNet50, ResNet101, Resnet152, VGG19, Inception V3\n",
        "  if cnn_model == 'InceptionV3':\n",
        "    model_fundus = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'VGG19':\n",
        "    model_fundus = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    model_fundus = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    model_fundus = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    model_fundus = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    model_fundus = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    model_fundus = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "\n",
        "  model_fundus._name = \"Level_1\"\n",
        "  model_ROI._name = \"Level_2\"\n",
        "\n",
        "  for layer in model_ROI.layers:\n",
        "      layer._name = layer.name + str(\"_2\")\n",
        "\n",
        "  Level_1 = model_fundus.output\n",
        "  Level_2 = model_ROI.output\n",
        "  #x = Concatenate(axis=-1)([Level_1, Level_2])#feature combination using concatenation\n",
        "  x = Add()([Level_1, Level_2])#feature combination using addition\n",
        "  x = Flatten()(x)\n",
        "  #MLP\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instanciar e compilar modelo\n",
        "  model = Model(inputs = [model_fundus.input,model_ROI.input], outputs = output_tensor)\n",
        "  opt = Adam(learning_rate=lr)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  #fn = dir_save + str(trial.number) + '_cnn.h5'#save models\n",
        "  fn = dir_save + str(trial.number) + '_cnn.hdf5'#save only weights\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, verbose=1, mode='auto'),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=0, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss', verbose=1,save_weights_only=True, save_best_only=True)]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=[x_train_fundus, x_train_ROI], y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          validation_data =([x_val_fundus, x_val_ROI],y_train)\n",
        "                          #validation_split=0.1,\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-Ec_iVizQA"
      },
      "source": [
        "#Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zerlCBMWgNn",
        "outputId": "b53a3928-3732-42db-a787-ec1eb617c410"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** starting at 2023-07-30 12:56:07.355279\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 9s 0us/step\n",
            "Epoch 1/150\n",
            "30/30 [==============================] - ETA: 0s - loss: 12.0079 - accuracy: 0.3889\n",
            "Epoch 1: val_loss improved from inf to 31064486.00000, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 105s 420ms/step - loss: 12.0079 - accuracy: 0.3889 - val_loss: 31064486.0000 - val_accuracy: 0.5000\n",
            "Epoch 2/150\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.4420 - accuracy: 0.4778\n",
            "Epoch 2: val_loss improved from 31064486.00000 to 6026.59131, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 10s 328ms/step - loss: 4.4420 - accuracy: 0.4778 - val_loss: 6026.5913 - val_accuracy: 0.2000\n",
            "Epoch 3/150\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.1201 - accuracy: 0.4556\n",
            "Epoch 3: val_loss improved from 6026.59131 to 17.75927, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 6s 209ms/step - loss: 1.1201 - accuracy: 0.4556 - val_loss: 17.7593 - val_accuracy: 0.2000\n",
            "Epoch 4/150\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0193 - accuracy: 0.5111\n",
            "Epoch 4: val_loss improved from 17.75927 to 1.03179, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 9s 315ms/step - loss: 1.0193 - accuracy: 0.5111 - val_loss: 1.0318 - val_accuracy: 0.5000\n",
            "Epoch 5/150\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.6780 - accuracy: 0.4556\n",
            "Epoch 5: val_loss did not improve from 1.03179\n",
            "30/30 [==============================] - 3s 95ms/step - loss: 1.6780 - accuracy: 0.4556 - val_loss: 1.0472 - val_accuracy: 0.5000\n",
            "Epoch 6/150\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.8672 - accuracy: 0.4556\n",
            "Epoch 6: val_loss did not improve from 1.03179\n",
            "30/30 [==============================] - 3s 94ms/step - loss: 1.8672 - accuracy: 0.4556 - val_loss: 441.4030 - val_accuracy: 0.2000\n",
            "Epoch 6: early stopping\n",
            "4/4 [==============================] - 4s 300ms/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234698864/234698864 [==============================] - 12s 0us/step\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 127.7619 - accuracy: 0.3333\n",
            "Epoch 1: val_loss improved from inf to 6753141593928105984.00000, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 135s 333ms/step - loss: 127.7619 - accuracy: 0.3333 - val_loss: 6753141593928105984.0000 - val_accuracy: 0.5000\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 12.5194 - accuracy: 0.4556\n",
            "Epoch 2: val_loss improved from 6753141593928105984.00000 to 18589554688.00000, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 13s 300ms/step - loss: 12.5194 - accuracy: 0.4556 - val_loss: 18589554688.0000 - val_accuracy: 0.5000\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 3.5154 - accuracy: 0.4556\n",
            "Epoch 3: val_loss improved from 18589554688.00000 to 2686821.25000, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 16s 368ms/step - loss: 3.5154 - accuracy: 0.4556 - val_loss: 2686821.2500 - val_accuracy: 0.5000\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 3.1535 - accuracy: 0.4333\n",
            "Epoch 4: val_loss improved from 2686821.25000 to 1.04554, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 21s 467ms/step - loss: 3.1535 - accuracy: 0.4333 - val_loss: 1.0455 - val_accuracy: 0.5000\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 3.8963 - accuracy: 0.4444\n",
            "Epoch 5: val_loss improved from 1.04554 to 1.04422, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 22s 487ms/step - loss: 3.8963 - accuracy: 0.4444 - val_loss: 1.0442 - val_accuracy: 0.5000\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 2.0762 - accuracy: 0.4667\n",
            "Epoch 6: val_loss did not improve from 1.04422\n",
            "45/45 [==============================] - 5s 111ms/step - loss: 2.0762 - accuracy: 0.4667 - val_loss: 1.0517 - val_accuracy: 0.5000\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 2.1807 - accuracy: 0.4444\n",
            "Epoch 7: val_loss did not improve from 1.04422\n",
            "45/45 [==============================] - 6s 135ms/step - loss: 2.1807 - accuracy: 0.4444 - val_loss: 1.0506 - val_accuracy: 0.5000\n",
            "Epoch 7: early stopping\n",
            "4/4 [==============================] - 4s 233ms/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29084464/29084464 [==============================] - 3s 0us/step\n",
            "Epoch 1/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 5.8381 - accuracy: 0.3222\n",
            "Epoch 1: val_loss improved from inf to 1.47036, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "90/90 [==============================] - 86s 123ms/step - loss: 5.8381 - accuracy: 0.3222 - val_loss: 1.4704 - val_accuracy: 0.5000\n",
            "Epoch 2/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 3.6240 - accuracy: 0.5111\n",
            "Epoch 2: val_loss did not improve from 1.47036\n",
            "90/90 [==============================] - 6s 71ms/step - loss: 3.6240 - accuracy: 0.5111 - val_loss: 2.1603 - val_accuracy: 0.3000\n",
            "Epoch 3/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 2.1220 - accuracy: 0.4778\n",
            "Epoch 3: val_loss did not improve from 1.47036\n",
            "90/90 [==============================] - 8s 84ms/step - loss: 2.1220 - accuracy: 0.4778 - val_loss: 1.6448 - val_accuracy: 0.2000\n",
            "Epoch 3: early stopping\n",
            "4/4 [==============================] - 6s 495ms/step\n",
            "Epoch 1/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 241.0812 - accuracy: 0.4000\n",
            "Epoch 1: val_loss improved from inf to 49791049812301316096.00000, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 94s 399ms/step - loss: 241.0812 - accuracy: 0.4000 - val_loss: 49791049812301316096.0000 - val_accuracy: 0.2000\n",
            "Epoch 2/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 21.7009 - accuracy: 0.4444\n",
            "Epoch 2: val_loss improved from 49791049812301316096.00000 to 6662407.00000, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 24s 548ms/step - loss: 21.7009 - accuracy: 0.4444 - val_loss: 6662407.0000 - val_accuracy: 0.5000\n",
            "Epoch 3/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 7.3673 - accuracy: 0.4556\n",
            "Epoch 3: val_loss improved from 6662407.00000 to 1.06227, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 28s 635ms/step - loss: 7.3673 - accuracy: 0.4556 - val_loss: 1.0623 - val_accuracy: 0.5000\n",
            "Epoch 4/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.5335 - accuracy: 0.4556\n",
            "Epoch 4: val_loss did not improve from 1.06227\n",
            "45/45 [==============================] - 4s 83ms/step - loss: 1.5335 - accuracy: 0.4556 - val_loss: 1.0682 - val_accuracy: 0.5000\n",
            "Epoch 5/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.1093 - accuracy: 0.4778\n",
            "Epoch 5: val_loss improved from 1.06227 to 1.06083, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 12s 281ms/step - loss: 1.1093 - accuracy: 0.4778 - val_loss: 1.0608 - val_accuracy: 0.5000\n",
            "Epoch 6/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.0727 - accuracy: 0.5000\n",
            "Epoch 6: val_loss improved from 1.06083 to 1.05141, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 15s 347ms/step - loss: 1.0727 - accuracy: 0.5000 - val_loss: 1.0514 - val_accuracy: 0.5000\n",
            "Epoch 7/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.0522 - accuracy: 0.5000\n",
            "Epoch 7: val_loss improved from 1.05141 to 1.04580, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 21s 473ms/step - loss: 1.0522 - accuracy: 0.5000 - val_loss: 1.0458 - val_accuracy: 0.5000\n",
            "Epoch 8/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.0619 - accuracy: 0.5000\n",
            "Epoch 8: val_loss improved from 1.04580 to 1.04552, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 20s 441ms/step - loss: 1.0619 - accuracy: 0.5000 - val_loss: 1.0455 - val_accuracy: 0.5000\n",
            "Epoch 9/150\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.6180 - accuracy: 0.4889\n",
            "Epoch 9: val_loss did not improve from 1.04552\n",
            "45/45 [==============================] - 5s 101ms/step - loss: 1.6180 - accuracy: 0.4889 - val_loss: 1.0492 - val_accuracy: 0.5000\n",
            "Epoch 9: early stopping\n",
            "4/4 [==============================] - 2s 166ms/step\n",
            "Epoch 1/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 6.7415 - accuracy: 0.4556\n",
            "Epoch 1: val_loss improved from inf to 1.93671, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 112s 378ms/step - loss: 6.7415 - accuracy: 0.4556 - val_loss: 1.9367 - val_accuracy: 0.2000\n",
            "Epoch 2/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.1403 - accuracy: 0.4778\n",
            "Epoch 2: val_loss improved from 1.93671 to 1.08596, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 32s 364ms/step - loss: 1.1403 - accuracy: 0.4778 - val_loss: 1.0860 - val_accuracy: 0.5000\n",
            "Epoch 3/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0841 - accuracy: 0.4889\n",
            "Epoch 3: val_loss improved from 1.08596 to 1.08157, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 25s 282ms/step - loss: 1.0841 - accuracy: 0.4889 - val_loss: 1.0816 - val_accuracy: 0.5000\n",
            "Epoch 4/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.2266 - accuracy: 0.4444\n",
            "Epoch 4: val_loss did not improve from 1.08157\n",
            "90/90 [==============================] - 7s 80ms/step - loss: 1.2266 - accuracy: 0.4444 - val_loss: 1.0844 - val_accuracy: 0.5000\n",
            "Epoch 5/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0757 - accuracy: 0.5000\n",
            "Epoch 5: val_loss improved from 1.08157 to 1.07607, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 28s 317ms/step - loss: 1.0757 - accuracy: 0.5000 - val_loss: 1.0761 - val_accuracy: 0.5000\n",
            "Epoch 6/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.1647 - accuracy: 0.4889\n",
            "Epoch 6: val_loss improved from 1.07607 to 1.06856, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 34s 376ms/step - loss: 1.1647 - accuracy: 0.4889 - val_loss: 1.0686 - val_accuracy: 0.5000\n",
            "Epoch 7/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0868 - accuracy: 0.5000\n",
            "Epoch 7: val_loss improved from 1.06856 to 1.06392, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 24s 265ms/step - loss: 1.0868 - accuracy: 0.5000 - val_loss: 1.0639 - val_accuracy: 0.5000\n",
            "Epoch 8/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.1105 - accuracy: 0.5000\n",
            "Epoch 8: val_loss improved from 1.06392 to 1.06081, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 33s 366ms/step - loss: 1.1105 - accuracy: 0.5000 - val_loss: 1.0608 - val_accuracy: 0.5000\n",
            "Epoch 9/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0622 - accuracy: 0.5000\n",
            "Epoch 9: val_loss improved from 1.06081 to 1.05806, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 29s 330ms/step - loss: 1.0622 - accuracy: 0.5000 - val_loss: 1.0581 - val_accuracy: 0.5000\n",
            "Epoch 10/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0596 - accuracy: 0.5000\n",
            "Epoch 10: val_loss improved from 1.05806 to 1.05457, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 27s 297ms/step - loss: 1.0596 - accuracy: 0.5000 - val_loss: 1.0546 - val_accuracy: 0.5000\n",
            "Epoch 11/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0568 - accuracy: 0.5000\n",
            "Epoch 11: val_loss improved from 1.05457 to 1.05307, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 27s 302ms/step - loss: 1.0568 - accuracy: 0.5000 - val_loss: 1.0531 - val_accuracy: 0.5000\n",
            "Epoch 12/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0547 - accuracy: 0.5000\n",
            "Epoch 12: val_loss improved from 1.05307 to 1.05286, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 26s 286ms/step - loss: 1.0547 - accuracy: 0.5000 - val_loss: 1.0529 - val_accuracy: 0.5000\n",
            "Epoch 13/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0530 - accuracy: 0.5000\n",
            "Epoch 13: val_loss improved from 1.05286 to 1.04968, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 28s 312ms/step - loss: 1.0530 - accuracy: 0.5000 - val_loss: 1.0497 - val_accuracy: 0.5000\n",
            "Epoch 14/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0515 - accuracy: 0.5000\n",
            "Epoch 14: val_loss improved from 1.04968 to 1.04945, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "90/90 [==============================] - 29s 327ms/step - loss: 1.0515 - accuracy: 0.5000 - val_loss: 1.0494 - val_accuracy: 0.5000\n",
            "Epoch 15/150\n",
            "90/90 [==============================] - ETA: 0s - loss: 1.0497 - accuracy: 0.5000\n",
            "Epoch 15: val_loss did not improve from 1.04945\n",
            "90/90 [==============================] - 8s 92ms/step - loss: 1.0497 - accuracy: 0.5000 - val_loss: 1.0525 - val_accuracy: 0.5000\n",
            "Epoch 15: early stopping\n",
            "4/4 [==============================] - 2s 170ms/step\n"
          ]
        }
      ],
      "source": [
        "val_path_dataset = '/content/Test_CNN/'\n",
        "os.chdir(val_path_dataset)\n",
        "\n",
        "if not Path(dir_save).is_dir():\n",
        "  os.mkdir(dir_save)\n",
        "\n",
        "print('\\n*** starting at',pd.Timestamp.now())\n",
        "start_time_total = time.time()\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(directions=optimizer_direction,study_name=\"starter-experiment\")\n",
        "study.optimize(objective, n_trials=5,gc_after_trial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbX03qiOGgV",
        "outputId": "85af87f1-ed66-45e9-fd63-f4b4954d8197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "total elapsed time = 9.191481061776479  minutes\n"
          ]
        }
      ],
      "source": [
        "# save results\n",
        "df_results = study.trials_dataframe()\n",
        "if not Path(results_dir).is_dir():\n",
        "  os.mkdir(results_dir)\n",
        "df_results.to_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "df_results.to_csv(results_dir + 'df_optuna_results.csv')\n",
        "elapsed_time_total = (time.time()-start_time_total)/60\n",
        "print('\\n\\ntotal elapsed time =',elapsed_time_total,' minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN_1Z9uNjmk"
      },
      "source": [
        "# Sort results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO7C-8OZNivN",
        "outputId": "9b27a060-6f88-41cc-c174-d8cb49dcf94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted CSV file (according to multiple columns) =     Unnamed: 0  number  values_0  values_1              datetime_start  \\\n",
            "2           2       2  0.924700       0.7  2023-07-30 00:40:38.980133   \n",
            "3           3       3  1.032561       0.5  2023-07-30 00:43:31.413669   \n",
            "4           4       4  1.039189       0.5  2023-07-30 00:44:01.420744   \n",
            "1           1       1  1.049983       0.5  2023-07-30 00:39:52.849290   \n",
            "0           0       0  3.275835       0.5  2023-07-30 00:38:13.830793   \n",
            "\n",
            "            datetime_complete                duration  params_batch_size  \\\n",
            "2  2023-07-30 00:43:30.218678  0 days 00:02:51.238545                  1   \n",
            "3  2023-07-30 00:44:01.053716  0 days 00:00:29.640047                  2   \n",
            "4  2023-07-30 00:45:18.021475  0 days 00:01:16.600731                  2   \n",
            "1  2023-07-30 00:40:24.659844  0 days 00:00:31.810554                  1   \n",
            "0  2023-07-30 00:39:26.512972  0 days 00:01:12.682179                  3   \n",
            "\n",
            "  params_cnn_model  params_dense_nodes_divisor  params_drop_out_rate  \\\n",
            "2      Densenet169                           2                   0.0   \n",
            "3            VGG19                           8                   0.0   \n",
            "4            VGG19                           4                   0.4   \n",
            "1            VGG19                           8                   0.2   \n",
            "0         resnet50                           4                   0.0   \n",
            "\n",
            "   params_lr  params_num_dense_nodes  params_num_layers  \\\n",
            "2     0.0001                     128                  1   \n",
            "3     0.0001                      64                  1   \n",
            "4     0.0010                     256                  1   \n",
            "1     0.0001                     128                  1   \n",
            "0     0.0001                     256                  2   \n",
            "\n",
            "   system_attrs_nsga2:generation     state  \n",
            "2                              0  COMPLETE  \n",
            "3                              0  COMPLETE  \n",
            "4                              0  COMPLETE  \n",
            "1                              0  COMPLETE  \n",
            "0                              0  COMPLETE  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "val_path_dataset = '/content/Test_CNN/Results_optuna/Test_'+teste_number#path where optuna dataframe was saved\n",
        "os.chdir(val_path_dataset)\n",
        "dataframe = pd.read_csv('df_optuna_results.csv')\n",
        "dataframe.sort_values(\"values_0\",axis=0,ascending=True, inplace=True, na_position='first')\n",
        "print(\"Sorted CSV file (according to multiple columns) = \", dataframe.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnDo77oT1MI"
      },
      "source": [
        "# Evaluate best models in split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FR0xpKTW1M0"
      },
      "source": [
        "Test with fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT9tHTTKQRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ca25ea-acad-43ee-a185-5104f3b69546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 101ms/step\n"
          ]
        }
      ],
      "source": [
        "best_model = keras.models.load_model('/content/Test_CNN/Models/Test_'+teste_number+'0_cnn.h5')\n",
        "prob_class = best_model.predict(x_test_fundus)#probabilities\n",
        "predicted_classes = np.argmax(prob_class, axis=1)\n",
        "gera_csv(predicted_classes, classification_results_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble of best models from optimization process"
      ],
      "metadata": {
        "id": "x7v-YIIsYDUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_final_predictions(xprod,\n",
        "                           models_directory,\n",
        "                           df_params,\n",
        "                           threshold, num_models_accept,\n",
        "                           optimization_direction):\n",
        "\n",
        "    if optimization_direction == 'maximize':\n",
        "        df_params.sort_values(by='values_1', ascending=False, inplace=True)\n",
        "    else:\n",
        "        df_params.sort_values(by='values_0', ascending=True, inplace=True)\n",
        "\n",
        "    # apply threshold\n",
        "    accepted_models_num = 0\n",
        "    list_predicted_prob = []\n",
        "    num_models_total = df_params.shape[0]\n",
        "    for i in range(num_models_total):\n",
        "        if optimization_direction == 'maximize':\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_1'] > threshold\n",
        "        else:\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_0'] < threshold\n",
        "\n",
        "        bool2 = df_params.loc[df_params.index[i],'state'] == 'COMPLETE'\n",
        "        bool3 = accepted_models_num < num_models_accept\n",
        "        if bool1 and bool2 and bool3:\n",
        "            model_number = str(df_params.loc[df_params.index[i],'number'])\n",
        "            #if model_number != '6':\n",
        "            print (model_number)\n",
        "            try:\n",
        "                cnn_model = keras.models.load_model(models_directory + model_number + '_cnn.h5')\n",
        "            except:\n",
        "                print('\\ncould not read model number:',model_number)\n",
        "            else:\n",
        "                list_predicted_prob.append(cnn_model.predict(xprod))\n",
        "                accepted_models_num = accepted_models_num + 1\n",
        "\n",
        "    # compute mean probabilities\n",
        "    mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "    # compute predicted class\n",
        "    # argmax uses 1st ocurrance in case of a tie\n",
        "    y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "    return y_predicted_class\n",
        "\n",
        "# fixed parameters - production (Ensemble)\n",
        "threshold_error =  3# validation loss\n",
        "number_of_models = 5\n",
        "\n",
        "# get optuna results parameters\n",
        "#models_dir = results_directory_stub + 'calibration/'\n",
        "df_parameters = pd.read_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "\n",
        "#results_directory = results_directory_stub + calculation_type + '/'\n",
        "#if not Path(results_directory).is_dir():\n",
        "    #os.mkdir(results_directory)\n",
        "\n",
        "final_result = make_final_predictions(x_test_fundus,\n",
        "                               dir_save,\n",
        "                               df_parameters,\n",
        "                               threshold_error,\n",
        "                               number_of_models, optimizer_direction)\n",
        "\n",
        "trial_number = 100\n",
        "gera_csv(final_result, classification_results_dir, str(trial_number))"
      ],
      "metadata": {
        "id": "uX3iZHVjYM7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4odujajzijHe",
        "qPN2YsxZmki0",
        "dP4xlwCWNFfn",
        "wKUezmqObFkP",
        "OjM_QudD7nP9",
        "uA0tFXlBC2hF",
        "QFkaOH2XWw69",
        "W4R9n8EEXRcq",
        "sC_a8OAJu7We",
        "9fmQ-6Rv2qTD",
        "C0HE_wEfLkWD",
        "gmuOfFOAf03E",
        "9PoS0_oRjGRZ",
        "BK4BQEDOMIyk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}