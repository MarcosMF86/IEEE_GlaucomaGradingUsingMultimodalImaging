{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx06mo_SI5sT"
      },
      "source": [
        "# Path to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duh7fkJVIFMJ",
        "outputId": "3d911ed2-26a1-46fb-a4f0-a57e4c8648aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_training_data/multi-modality_images/'#path to dataset\n",
        "os.chdir(path_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFI8Ds2VJMvB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGTsyeK7a6nT"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkg1_ngQTDcx"
      },
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwqbK7PSUDG"
      },
      "source": [
        "# 3D CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mylq9tzuSX4O"
      },
      "outputs": [],
      "source": [
        "!pip install classification-models-3D\n",
        "!pip install keras_applications\n",
        "from classification_models_3D.tfkeras import Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3A4VA8dv05n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from collections import Counter#usar para contar a quantidade de uma classe ex: counter(y_train)\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aGk1GntJPWa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from io import BytesIO\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, Concatenate, Reshape, GlobalMaxPooling2D, GlobalMaxPooling3D, GlobalAveragePooling3D, Conv2D, Conv1D, Add\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score, roc_curve,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import random\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from tensorflow.keras import layers\n",
        "import csv\n",
        "from utils_GAMMA_V2 import padroniza_resultado\n",
        "from utils_GAMMA_V2 import converte\n",
        "from scipy import stats\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Sequential, Model\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from keras.backend import int_shape\n",
        "import pickle\n",
        "import optuna\n",
        "from keras.applications.densenet import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_LCC90hOA38"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1Dzcr6_OEWH"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224 #---> Definido no arquivo importado vit.py\n",
        "depth = 64\n",
        "# TRAINING\n",
        "EPOCHS = 150\n",
        "early_stop_epochs = 2\n",
        "learning_rate_epochs = 5\n",
        "optimizer_direction = ['minimize', \"maximize\"]\n",
        "number_of_random_points = 25  # random searches to start opt process\n",
        "#Foldes to save models and dataframes results\n",
        "teste_number = '1/'\n",
        "dir_save = '/content/Test_CNN/Models/Test_' + teste_number\n",
        "results_dir = '/content/Test_CNN/Results_optuna/Test_' + teste_number\n",
        "classification_results_dir = '/content/Test_CNN/Classification Results Optuna/Test_' + teste_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nChTJ8nZIeaE"
      },
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qszb6AnZIeu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f60e9f0-812a-4840-d009-c9a91aed5d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "folder_fundus = 'dataset_fundus/train/fundus_images flip - 100 images/'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "\n",
        "conj = 0# 0--> load train images; 1--> load val images\n",
        "X_train_fundus,X_train_ROI,Y = load_fundus_images(path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)\n",
        "num_classes = len(np.unique(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sH0qrRfCK2H"
      },
      "source": [
        "Read OCTs Volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rl69maz-CKJB"
      },
      "outputs": [],
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "scan_paths = dir_octs(path_dataset,'multi-modality_images',0)#path to 3D volumes\n",
        "X_oct = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read 3D volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfWV4riR5cOX"
      },
      "source": [
        "Split Train/Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundus"
      ],
      "metadata": {
        "id": "bHzmMo2Pwner"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJaA-9Q65feN",
        "outputId": "f73ed4f8-7f7f-409b-d99a-780d0a8c073f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_fundus, x_val_fundus, y_train, y_val = train_test_split(X_train_fundus, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_fundus.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_fundus.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdt6hjh47CB2"
      },
      "source": [
        "ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRcvKYwe6t8o",
        "outputId": "6b41e826-b0f7-451b-f75c-abd0bb1e2d20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_ROI, x_val_ROI, y_train, y_val = train_test_split(X_train_ROI, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_ROI.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_ROI.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXteIGLv7ExN"
      },
      "source": [
        "OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqdcH2sue_mY"
      },
      "outputs": [],
      "source": [
        "x_train_oct, x_val_oct, y_train, y_val = train_test_split(X_oct, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_oct.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_oct.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Test"
      ],
      "metadata": {
        "id": "70gIQIWCyuC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-h-gFAFTTov",
        "outputId": "2f7de93e-3ea0-4129-bfbc-ed52f672cf96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "x_teste shape: (100, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "folder_fundus = 'fundus_images'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "conj = 1# 1-->Load Images from test set\n",
        "x_test_fundus,x_test_ROI = load_fundus_images(val_path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)#\n",
        "print(f\"x_teste shape: {x_test_fundus.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read OCT Volumes"
      ],
      "metadata": {
        "id": "boD-pbzx0mKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "conj = 1#conj = 0 (Train); conj = 1(Test)\n",
        "scan_paths = dir_octs(val_path_dataset,'multi-modality_images',conj)\n",
        "#scan_paths = scan_paths[:1]\n",
        "X_test_OCT = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read volumes"
      ],
      "metadata": {
        "id": "JP5QL2Kg0lcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVcEwSxPWRh"
      },
      "source": [
        "# Plot Fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hmDQk3XPYRy"
      },
      "outputs": [],
      "source": [
        " # number of images\n",
        "n_images = 100\n",
        "\n",
        "# arrays\n",
        "x = X_train_fundus\n",
        "y = Y\n",
        "\n",
        "# numer of rows and columns\n",
        "rows = 10\n",
        "cols = 10\n",
        "\n",
        "# empty figure\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(28, 28))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# para cada imagem\n",
        "for i in range(n_images):\n",
        "    #subplot\n",
        "    axs[i].imshow(x[i], cmap='gray')\n",
        "    axs[i].set_title(f'Label: {y[i]}')\n",
        "    # remove as bordas e o eixo x, y\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate CSV to be evaluated online"
      ],
      "metadata": {
        "id": "4A64nNEz2F3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Gerar o CSV com os resultados obtidos no slit de validação\n",
        "def gera_csv(predicted_classes, classification_results_dir, trial_number):\n",
        "  #classification_results_dir = 'content/Test_CNN/Classification Results Optuna/Test_'+teste_number\n",
        "  if not Path(classification_results_dir).is_dir():\n",
        "    os.mkdir(classification_results_dir)\n",
        "  header = ['data','non','early','mid_advanced']\n",
        "  results = padroniza_resultado(predicted_classes)\n",
        "  #Alterar a pasta\n",
        "  results_file_path = os.path.join(val_path_dataset, classification_results_dir  + '/Classification_Results_'+trial_number+'.csv')\n",
        "  with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)"
      ],
      "metadata": {
        "id": "CFQTdFCY2G-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dP4xlwCWNFfn"
      },
      "source": [
        "# Optimize 2Level Models - V2\n",
        "Load pre-trained one level models without classifiers.\n",
        "Freeze some convolutional blocks\n",
        "New FC Layers are trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDJikRhZNBSY"
      },
      "outputs": [],
      "source": [
        "def objective(trial):#cria e avalia modelo\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  freeze_layers = trial.suggest_categorical('freeze_layers',[10, 15, 20, 23])#depends on the number of layers in the model\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001,])\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])\n",
        "  #cnn_model = trial.suggest_categorical('cnn_model', ['InceptionV3','VGG19','resnet50', 'resnet101', 'resnet152', 'Densenet121', 'Densenet169'])\n",
        "  #CNNs: Dense121, Dense 169, ResNet50, ResNet101, Resnet152, VGG19, Inception V3\n",
        "\n",
        "  #best model using fundus images as input\n",
        "  best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_1/22_cnn.h5')#resolution 128x128\n",
        "  #best model using ROI as input\n",
        "  best_model_ROI = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_2A/16_cnn.h5')#Resolution 128x128\n",
        "\n",
        "  for layer in best_model_ROI.layers:\n",
        "    layer._name = layer.name + str(\"_2\")\n",
        "\n",
        "  best_model_fundus._name = \"Level_1\"\n",
        "  best_model_ROI._name = \"Level_2\"\n",
        "\n",
        "  #best_model_fundus.trainable = False #Weights will not be updated\n",
        "  #best_model_ROI.trainable = False #Weights will not be updated\n",
        "\n",
        "  #freeze convolutions blocks\n",
        "  for layer in best_model_fundus.layers[:-freeze_layers]:\n",
        "    layer.trainable= False\n",
        "\n",
        "  #for layer in best_model_fundus.layers:\n",
        "    #print(layer.trainable)\n",
        "\n",
        "  for layer in best_model_ROI.layers[:-freeze_layers]:\n",
        "    layer.trainable= False\n",
        "\n",
        "  Level_1 = best_model_fundus.layers[-6].output#Eliminate MLP\n",
        "  Level_2 = best_model_ROI.layers[-6].output#Eliminate MLP\n",
        "  #Level_1 = Flatten()(Level_1)\n",
        "  #Level_2 = Flatten()(Level_2)\n",
        "  network = Concatenate(axis=-1)([Level_1, Level_2])\n",
        "  #network = Add()([Level_1, Level_2])\n",
        "  network = Flatten()(network)\n",
        "  network = Dense(num_dense_nodes, activation='relu')(network)\n",
        "  network = Dropout(drop_out_rate)(network)\n",
        "  if num_layers == 2:\n",
        "    network = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(network)\n",
        "    network = Dropout(drop_out_rate)(network)\n",
        "  predictions = Dense(num_classes, activation='softmax')(network)\n",
        "  model = Model(inputs = [best_model_fundus.input, best_model_ROI.input], outputs = predictions)\n",
        "\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy',])#loss='sparse_categorical_crossentropy'\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  fn = dir_save + str(trial.number) + '_cnn.hdf5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, verbose=1, mode='auto'),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=0, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss',verbose=1,save_weights_only=True,save_best_only=True)\n",
        "                    ]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=[x_train_fundus, x_train_ROI], y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = ([x_val_fundus, x_val_ROI],y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPN2YsxZmki0"
      },
      "source": [
        "# Optimize 2Level Models -V1\n",
        "Models Load with Imagenet Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmRAzz7ahk2k"
      },
      "outputs": [],
      "source": [
        "def objective(trial):#cria e avalia modelo\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['InceptionV3','VGG19','resnet50', 'resnet101', 'resnet152', 'Densenet121', 'Densenet169'])\n",
        "  #CNNs: Dense121, Dense 169, ResNet50, ResNet101, Resnet152, VGG19, Inception V3\n",
        "  if cnn_model == 'InceptionV3':\n",
        "    model_fundus = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'VGG19':\n",
        "    model_fundus = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    model_fundus = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    model_fundus = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    model_fundus = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    model_fundus = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    model_fundus = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "    model_ROI = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "\n",
        "  model_fundus._name = \"Level_1\"\n",
        "  model_ROI._name = \"Level_2\"\n",
        "\n",
        "  for layer in model_ROI.layers:\n",
        "      layer._name = layer.name + str(\"_2\")\n",
        "\n",
        "  Level_1 = model_fundus.output\n",
        "  Level_2 = model_ROI.output\n",
        "  #x = Concatenate(axis=-1)([Level_1, Level_2])#feature combination using concatenation\n",
        "  x = Add()([Level_1, Level_2])#feature combination using addition\n",
        "  x = Flatten()(x)\n",
        "  #MLP\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instanciar e compilar modelo\n",
        "  model = Model(inputs = [model_fundus.input,model_ROI.input], outputs = output_tensor)\n",
        "  opt = Adam(learning_rate=lr)\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  #fn = dir_save + str(trial.number) + '_cnn.h5'#save models\n",
        "  fn = dir_save + str(trial.number) + '_cnn.hdf5'#save only weights\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, verbose=1, mode='auto'),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=0, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss', verbose=1,save_weights_only=True, save_best_only=True)]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=[x_train_fundus, x_train_ROI], y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          validation_data =([x_val_fundus, x_val_ROI],y_train)\n",
        "                          #validation_split=0.1,\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-Ec_iVizQA"
      },
      "source": [
        "#Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zerlCBMWgNn"
      },
      "outputs": [],
      "source": [
        "val_path_dataset = '/content/Test_CNN/'\n",
        "os.chdir(val_path_dataset)\n",
        "\n",
        "if not Path(dir_save).is_dir():\n",
        "  os.mkdir(dir_save)\n",
        "\n",
        "print('\\n*** starting at',pd.Timestamp.now())\n",
        "start_time_total = time.time()\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(directions=optimizer_direction,study_name=\"starter-experiment\")\n",
        "study.optimize(objective, n_trials=5,gc_after_trial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbX03qiOGgV",
        "outputId": "85af87f1-ed66-45e9-fd63-f4b4954d8197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "total elapsed time = 9.191481061776479  minutes\n"
          ]
        }
      ],
      "source": [
        "# save results\n",
        "df_results = study.trials_dataframe()\n",
        "if not Path(results_dir).is_dir():\n",
        "  os.mkdir(results_dir)\n",
        "df_results.to_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "df_results.to_csv(results_dir + 'df_optuna_results.csv')\n",
        "elapsed_time_total = (time.time()-start_time_total)/60\n",
        "print('\\n\\ntotal elapsed time =',elapsed_time_total,' minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN_1Z9uNjmk"
      },
      "source": [
        "# Sort results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO7C-8OZNivN",
        "outputId": "9b27a060-6f88-41cc-c174-d8cb49dcf94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted CSV file (according to multiple columns) =     Unnamed: 0  number  values_0  values_1              datetime_start  \\\n",
            "2           2       2  0.924700       0.7  2023-07-30 00:40:38.980133   \n",
            "3           3       3  1.032561       0.5  2023-07-30 00:43:31.413669   \n",
            "4           4       4  1.039189       0.5  2023-07-30 00:44:01.420744   \n",
            "1           1       1  1.049983       0.5  2023-07-30 00:39:52.849290   \n",
            "0           0       0  3.275835       0.5  2023-07-30 00:38:13.830793   \n",
            "\n",
            "            datetime_complete                duration  params_batch_size  \\\n",
            "2  2023-07-30 00:43:30.218678  0 days 00:02:51.238545                  1   \n",
            "3  2023-07-30 00:44:01.053716  0 days 00:00:29.640047                  2   \n",
            "4  2023-07-30 00:45:18.021475  0 days 00:01:16.600731                  2   \n",
            "1  2023-07-30 00:40:24.659844  0 days 00:00:31.810554                  1   \n",
            "0  2023-07-30 00:39:26.512972  0 days 00:01:12.682179                  3   \n",
            "\n",
            "  params_cnn_model  params_dense_nodes_divisor  params_drop_out_rate  \\\n",
            "2      Densenet169                           2                   0.0   \n",
            "3            VGG19                           8                   0.0   \n",
            "4            VGG19                           4                   0.4   \n",
            "1            VGG19                           8                   0.2   \n",
            "0         resnet50                           4                   0.0   \n",
            "\n",
            "   params_lr  params_num_dense_nodes  params_num_layers  \\\n",
            "2     0.0001                     128                  1   \n",
            "3     0.0001                      64                  1   \n",
            "4     0.0010                     256                  1   \n",
            "1     0.0001                     128                  1   \n",
            "0     0.0001                     256                  2   \n",
            "\n",
            "   system_attrs_nsga2:generation     state  \n",
            "2                              0  COMPLETE  \n",
            "3                              0  COMPLETE  \n",
            "4                              0  COMPLETE  \n",
            "1                              0  COMPLETE  \n",
            "0                              0  COMPLETE  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "val_path_dataset = '/content/Test_CNN/Results_optuna/Test_'+teste_number#path where optuna dataframe was saved\n",
        "os.chdir(val_path_dataset)\n",
        "dataframe = pd.read_csv('df_optuna_results.csv')\n",
        "dataframe.sort_values(\"values_0\",axis=0,ascending=True, inplace=True, na_position='first')\n",
        "print(\"Sorted CSV file (according to multiple columns) = \", dataframe.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnDo77oT1MI"
      },
      "source": [
        "# Evaluate best models in split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FR0xpKTW1M0"
      },
      "source": [
        "Test with fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qT9tHTTKQRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ca25ea-acad-43ee-a185-5104f3b69546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 101ms/step\n"
          ]
        }
      ],
      "source": [
        "best_model = keras.models.load_model('/content/Test_CNN/Models/Test_'+teste_number+'0_cnn.h5')\n",
        "prob_class = best_model.predict(x_test_fundus)#probabilities\n",
        "predicted_classes = np.argmax(prob_class, axis=1)\n",
        "gera_csv(predicted_classes, classification_results_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble of best models from optimization process"
      ],
      "metadata": {
        "id": "x7v-YIIsYDUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_final_predictions(xprod,\n",
        "                           models_directory,\n",
        "                           df_params,\n",
        "                           threshold, num_models_accept,\n",
        "                           optimization_direction):\n",
        "\n",
        "    if optimization_direction == 'maximize':\n",
        "        df_params.sort_values(by='values_1', ascending=False, inplace=True)\n",
        "    else:\n",
        "        df_params.sort_values(by='values_0', ascending=True, inplace=True)\n",
        "\n",
        "    # apply threshold\n",
        "    accepted_models_num = 0\n",
        "    list_predicted_prob = []\n",
        "    num_models_total = df_params.shape[0]\n",
        "    for i in range(num_models_total):\n",
        "        if optimization_direction == 'maximize':\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_1'] > threshold\n",
        "        else:\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_0'] < threshold\n",
        "\n",
        "        bool2 = df_params.loc[df_params.index[i],'state'] == 'COMPLETE'\n",
        "        bool3 = accepted_models_num < num_models_accept\n",
        "        if bool1 and bool2 and bool3:\n",
        "            model_number = str(df_params.loc[df_params.index[i],'number'])\n",
        "            #if model_number != '6':\n",
        "            print (model_number)\n",
        "            try:\n",
        "                cnn_model = keras.models.load_model(models_directory + model_number + '_cnn.h5')\n",
        "            except:\n",
        "                print('\\ncould not read model number:',model_number)\n",
        "            else:\n",
        "                list_predicted_prob.append(cnn_model.predict(xprod))\n",
        "                accepted_models_num = accepted_models_num + 1\n",
        "\n",
        "    # compute mean probabilities\n",
        "    mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "    # compute predicted class\n",
        "    # argmax uses 1st ocurrance in case of a tie\n",
        "    y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "    return y_predicted_class\n",
        "\n",
        "# fixed parameters - production (Ensemble)\n",
        "threshold_error =  3# validation loss\n",
        "number_of_models = 5\n",
        "\n",
        "# get optuna results parameters\n",
        "#models_dir = results_directory_stub + 'calibration/'\n",
        "df_parameters = pd.read_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "\n",
        "#results_directory = results_directory_stub + calculation_type + '/'\n",
        "#if not Path(results_directory).is_dir():\n",
        "    #os.mkdir(results_directory)\n",
        "\n",
        "final_result = make_final_predictions(x_test_fundus,\n",
        "                               dir_save,\n",
        "                               df_parameters,\n",
        "                               threshold_error,\n",
        "                               number_of_models, optimizer_direction)\n",
        "\n",
        "trial_number = 100\n",
        "gera_csv(final_result, classification_results_dir, str(trial_number))"
      ],
      "metadata": {
        "id": "uX3iZHVjYM7c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4odujajzijHe",
        "qPN2YsxZmki0",
        "dP4xlwCWNFfn",
        "wKUezmqObFkP",
        "OjM_QudD7nP9",
        "uA0tFXlBC2hF",
        "QFkaOH2XWw69",
        "W4R9n8EEXRcq",
        "sC_a8OAJu7We",
        "9fmQ-6Rv2qTD",
        "C0HE_wEfLkWD",
        "gmuOfFOAf03E",
        "9PoS0_oRjGRZ",
        "BK4BQEDOMIyk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}