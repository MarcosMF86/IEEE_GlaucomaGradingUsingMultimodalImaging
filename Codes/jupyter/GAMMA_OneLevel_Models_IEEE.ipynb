{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx06mo_SI5sT"
      },
      "source": [
        "# Path to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duh7fkJVIFMJ",
        "outputId": "c04e176b-f24e-4378-b78f-18a1662da6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_training_data/multi-modality_images/'#path to dataset\n",
        "os.chdir(path_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFI8Ds2VJMvB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGTsyeK7a6nT",
        "outputId": "4fd65511-1c2f-457c-dabc-34072c7cbacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (612 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m612.1/612.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.21.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xkg1_ngQTDcx",
        "outputId": "b315378b-68b9-48e0-eac3-134e48541336"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n",
            "Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n",
            "Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.10.0 colorlog-6.7.0 optuna-3.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwqbK7PSUDG"
      },
      "source": [
        "# 3D CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mylq9tzuSX4O",
        "outputId": "85f8f2e5-e8a9-4687-f19d-feb19ff18891"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting classification-models-3D\n",
            "  Downloading classification_models_3D-1.0.7-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.5/69.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: classification-models-3D\n",
            "Successfully installed classification-models-3D-1.0.7\n",
            "Collecting keras_applications\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_applications) (1.22.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras_applications) (3.8.0)\n",
            "Installing collected packages: keras_applications\n",
            "Successfully installed keras_applications-1.0.8\n"
          ]
        }
      ],
      "source": [
        "!pip install classification-models-3D\n",
        "!pip install keras_applications\n",
        "from classification_models_3D.tfkeras import Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3A4VA8dv05n",
        "outputId": "f44f91d5-4695-442b-e0bf-41bcf96c3fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow Version 2.12.0\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9aGk1GntJPWa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from io import BytesIO\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, Concatenate, Reshape, GlobalMaxPooling2D, GlobalMaxPooling3D, GlobalAveragePooling3D, Conv2D, Conv1D, Add\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score, roc_curve,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import random\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from tensorflow.keras import layers\n",
        "import csv\n",
        "from utils_GAMMA_V2 import padroniza_resultado\n",
        "from utils_GAMMA_V2 import converte\n",
        "from scipy import stats\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Sequential, Model\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from keras.backend import int_shape\n",
        "import pickle\n",
        "import optuna\n",
        "from keras.applications.densenet import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_LCC90hOA38"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "x1Dzcr6_OEWH"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "depth = 64\n",
        "# TRAINING\n",
        "EPOCHS = 15\n",
        "early_stop_epochs = 2\n",
        "learning_rate_epochs = 5\n",
        "optimizer_direction = ['minimize', \"maximize\"]\n",
        "#Foldes to save models and dataframes results\n",
        "teste_number = '1/'\n",
        "dir_save = '/content/Test_CNN/Models/Test_' + teste_number\n",
        "results_dir = '/content/Test_CNN/Results_optuna/Test_' + teste_number\n",
        "classification_results_dir = '/content/Test_CNN/Classification Results Optuna/Test_' + teste_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nChTJ8nZIeaE"
      },
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qszb6AnZIeu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145dc3b3-2174-4261-a3e2-eea6540d342b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "folder_fundus = 'dataset_fundus/train/fundus_images flip - 100 images/'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "\n",
        "conj = 0# 0--> load train images; 1--> load test images\n",
        "X_train_fundus,X_train_ROI,Y = load_fundus_images(path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)\n",
        "num_classes = len(np.unique(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sH0qrRfCK2H"
      },
      "source": [
        "Read OCTs Volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Rl69maz-CKJB"
      },
      "outputs": [],
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "scan_paths = dir_octs(path_dataset,'multi-modality_images',0)#path to 3D volumes\n",
        "X_oct = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read 3D volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfWV4riR5cOX"
      },
      "source": [
        "Split Train/Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundus"
      ],
      "metadata": {
        "id": "bHzmMo2Pwner"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJaA-9Q65feN",
        "outputId": "d6a1c7e2-577e-4662-b879-2c6bf9212367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_fundus, x_val_fundus, y_train, y_val = train_test_split(X_train_fundus, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_fundus.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_fundus.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdt6hjh47CB2"
      },
      "source": [
        "ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRcvKYwe6t8o",
        "outputId": "dafc1ee8-812c-44cb-b157-04472f734fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_ROI, x_val_ROI, y_train, y_val = train_test_split(X_train_ROI, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_ROI.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_ROI.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXteIGLv7ExN"
      },
      "source": [
        "OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqdcH2sue_mY"
      },
      "outputs": [],
      "source": [
        "x_train_oct, x_val_oct, y_train, y_val = train_test_split(X_oct, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_oct.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_oct.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Test"
      ],
      "metadata": {
        "id": "70gIQIWCyuC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-h-gFAFTTov",
        "outputId": "493c750b-89dc-4468-a00c-8a172125b94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "x_teste shape: (100, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "folder_fundus = 'fundus_images'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "conj = 1# 1-->Load Images from test set\n",
        "x_test_fundus,x_test_ROI = load_fundus_images(val_path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)#\n",
        "print(f\"x_teste shape: {x_test_fundus.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read OCT Volumes"
      ],
      "metadata": {
        "id": "boD-pbzx0mKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "conj = 1#conj = 0 (Train); conj = 1(Test)\n",
        "scan_paths = dir_octs(val_path_dataset,'multi-modality_images',conj)\n",
        "#scan_paths = scan_paths[:1]\n",
        "X_test_OCT = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read volumes"
      ],
      "metadata": {
        "id": "JP5QL2Kg0lcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVcEwSxPWRh"
      },
      "source": [
        "# Plot Fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hmDQk3XPYRy"
      },
      "outputs": [],
      "source": [
        " # number of images\n",
        "n_images = 100\n",
        "\n",
        "# arrays\n",
        "x = X_train_fundus\n",
        "y = Y\n",
        "\n",
        "# numer of rows and columns\n",
        "rows = 10\n",
        "cols = 10\n",
        "\n",
        "# empty figure\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(28, 28))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# para cada imagem\n",
        "for i in range(n_images):\n",
        "    #subplot\n",
        "    axs[i].imshow(x[i], cmap='gray')\n",
        "    axs[i].set_title(f'Label: {y[i]}')\n",
        "    # remove as bordas e o eixo x, y\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate CSV to be evaluated online"
      ],
      "metadata": {
        "id": "4A64nNEz2F3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gera_csv(predicted_classes, classification_results_dir, trial_number):\n",
        "  #classification_results_dir = 'content/Test_CNN/Classification Results Optuna/Test_'+teste_number\n",
        "  if not Path(classification_results_dir).is_dir():\n",
        "    os.mkdir(classification_results_dir)\n",
        "  header = ['data','non','early','mid_advanced']\n",
        "  results = padroniza_resultado(predicted_classes)\n",
        "  #Alterar a pasta\n",
        "  results_file_path = os.path.join(val_path_dataset, classification_results_dir  + '/Classification_Results_'+trial_number+'.csv')\n",
        "  with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)"
      ],
      "metadata": {
        "id": "CFQTdFCY2G-Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4m5hBnEsNyn"
      },
      "source": [
        "# Optimize One-level models; Inputs: Fundus or ROI Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0nM2vUymiWO"
      },
      "source": [
        "1 level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c7UcFsLAL392"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001,])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['VGG19', 'resnet50', 'resnet101','resnet152','Densenet121','Densenet169'])\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])#fully connected layers\n",
        "  #i = tf.keras.layers.Input([IMAGE_SIZE,IMAGE_SIZE, 3], dtype = tf.uint8)\n",
        "  #x = tf.cast(i, tf.float32)\n",
        "\n",
        "  if cnn_model == 'InceptionV3':\n",
        "    model_fundus = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'VGG19':\n",
        "    model_fundus = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    model_fundus = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    model_fundus = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    model_fundus = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    model_fundus = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    model_fundus = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "\n",
        "  x = model_fundus.output\n",
        "  x = Flatten()(x)\n",
        "  #MLP\n",
        "  #x = Dense(num_dense_nodes, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  if num_layers == 2:\n",
        "    #x = Dense(num_dense_nodes//dense_nodes_divisor, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "    x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "    x = Dropout(drop_out_rate)(x)\n",
        "\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instantiate and compile model\n",
        "  model = Model(inputs=model_fundus.input,outputs=output_tensor)\n",
        "\n",
        "  #model = tf.keras.Model(inputs=[i], outputs=[output_tensor])\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping, learning rate reducer and model checkpoint\n",
        "  fn = dir_save + str(trial.number) + '_cnn.h5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, mode='auto', min_delta=1e-3,verbose=1),\n",
        "                     #MyThresholdCallback(threshold=0.95),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=1, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss', verbose=1,save_best_only=True),\n",
        "                    #PlotLossesKeras()\n",
        "                    ]\n",
        "\n",
        "  # fit the model (x_fundus or x_ROI)\n",
        "  h = model.fit(x=x_train_ROI, y=y_train,\n",
        "                #x=X_train_fundus, y = Y,#\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = (x_val_ROI,y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odujajzijHe"
      },
      "source": [
        "# Optimize One-level models; Inputs: 3D OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W48lSRb2inFX"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [32, 64, 128, 256])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['VGG19', 'resnet50', 'resnet101','resnet152','Densenet121','Densenet169'])\n",
        "  canais = 3\n",
        "\n",
        "  if cnn_model == 'InceptionV3':\n",
        "    Rede3D, preprocess_input = Classifiers.get('inceptionv3')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'VGG19':\n",
        "    Rede3D, preprocess_input = Classifiers.get('vgg19')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    Rede3D, preprocess_input = Classifiers.get('resnet50')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    Rede3D, preprocess_input = Classifiers.get('resnet101')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    Rede3D, preprocess_input = Classifiers.get('resnet152')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    Rede3D, preprocess_input = Classifiers.get('densenet121')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    Rede3D, preprocess_input = Classifiers.get('densenet169')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  x = model_oct.output\n",
        "  x = Flatten()(x)\n",
        "  #MLP\n",
        "  #x = Dense(num_dense_nodes, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  if num_layers == 2:\n",
        "    #x = Dense(num_dense_nodes//dense_nodes_divisor, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),activation='relu')(x)\n",
        "    x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "    x = Dropout(drop_out_rate)(x)\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instantiate and compile model\n",
        "  model = Model(inputs=model_oct.input,outputs=output_tensor)\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  fn = dir_save + str(trial.number) + '_cnn.h5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, mode='auto',verbose=1),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=1, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss',verbose=1, save_best_only=True),\n",
        "                    ]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=x_train_oct, y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = (x_val_oct,y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-Ec_iVizQA"
      },
      "source": [
        "#Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zerlCBMWgNn",
        "outputId": "5dfcf65d-cb8a-445b-dad0-1a3cf42d96b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "*** starting at 2023-07-31 13:54:18.690283\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet152_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234698864/234698864 [==============================] - 1s 0us/step\n",
            "Epoch 1/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 93.2397 - accuracy: 0.3667\n",
            "Epoch 1: val_loss did not improve from inf\n",
            "30/30 [==============================] - 138s 265ms/step - loss: 93.2397 - accuracy: 0.3667 - val_loss: nan - val_accuracy: 0.5000\n",
            "Epoch 2/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 16.2536 - accuracy: 0.4000\n",
            "Epoch 2: val_loss improved from inf to 28263679282452690370560.00000, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 12s 398ms/step - loss: 16.2536 - accuracy: 0.4000 - val_loss: 28263679282452690370560.0000 - val_accuracy: 0.5000\n",
            "Epoch 3/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 13.1013 - accuracy: 0.5000\n",
            "Epoch 3: val_loss improved from 28263679282452690370560.00000 to 871057619681280.00000, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 11s 386ms/step - loss: 13.1013 - accuracy: 0.5000 - val_loss: 871057619681280.0000 - val_accuracy: 0.2000\n",
            "Epoch 4/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 4.4116 - accuracy: 0.4889\n",
            "Epoch 4: val_loss improved from 871057619681280.00000 to 467673792.00000, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 21s 712ms/step - loss: 4.4116 - accuracy: 0.4889 - val_loss: 467673792.0000 - val_accuracy: 0.3000\n",
            "Epoch 5/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.8907 - accuracy: 0.4667\n",
            "Epoch 5: val_loss improved from 467673792.00000 to 29109.00391, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 20s 681ms/step - loss: 2.8907 - accuracy: 0.4667 - val_loss: 29109.0039 - val_accuracy: 0.3000\n",
            "Epoch 6/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.0737 - accuracy: 0.5333\n",
            "Epoch 6: val_loss improved from 29109.00391 to 1.04102, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 17s 582ms/step - loss: 2.0737 - accuracy: 0.5333 - val_loss: 1.0410 - val_accuracy: 0.5000\n",
            "Epoch 7/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.7177 - accuracy: 0.5333\n",
            "Epoch 7: val_loss improved from 1.04102 to 1.03987, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 14s 482ms/step - loss: 1.7177 - accuracy: 0.5333 - val_loss: 1.0399 - val_accuracy: 0.5000\n",
            "Epoch 8/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.4521 - accuracy: 0.4556\n",
            "Epoch 8: val_loss improved from 1.03987 to 1.03908, saving model to /content/Test_CNN/Models/Test_1/0_cnn.h5\n",
            "30/30 [==============================] - 28s 954ms/step - loss: 1.4521 - accuracy: 0.4556 - val_loss: 1.0391 - val_accuracy: 0.5000\n",
            "Epoch 9/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0967 - accuracy: 0.4889\n",
            "Epoch 9: val_loss did not improve from 1.03908\n",
            "30/30 [==============================] - 5s 156ms/step - loss: 1.0967 - accuracy: 0.4889 - val_loss: 1.0413 - val_accuracy: 0.5000\n",
            "Epoch 9: early stopping\n",
            "4/4 [==============================] - 5s 326ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2023-07-31 13:58:56,803] Trial 0 failed with parameters: {'num_dense_nodes': 256, 'lr': 0.01, 'dense_nodes_divisor': 2, 'batch_size': 3, 'drop_out_rate': 0.4, 'cnn_model': 'resnet152', 'num_layers': 2} because of the following error: The value nan is not acceptable..\n",
            "[W 2023-07-31 13:58:56,804] Trial 0 failed with value (nan, 0.5).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80134624/80134624 [==============================] - 1s 0us/step\n",
            "Epoch 1/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 104502083584.0000 - accuracy: 0.3889\n",
            "Epoch 1: val_loss improved from inf to 10348439.00000, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 13s 101ms/step - loss: 104502083584.0000 - accuracy: 0.3889 - val_loss: 10348439.0000 - val_accuracy: 0.5000\n",
            "Epoch 2/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 169901296.0000 - accuracy: 0.3111\n",
            "Epoch 2: val_loss improved from 10348439.00000 to 1065271.12500, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 5s 105ms/step - loss: 169901296.0000 - accuracy: 0.3111 - val_loss: 1065271.1250 - val_accuracy: 0.5000\n",
            "Epoch 3/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 134409.6094 - accuracy: 0.3111\n",
            "Epoch 3: val_loss improved from 1065271.12500 to 4318.79004, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 5s 114ms/step - loss: 134409.6094 - accuracy: 0.3111 - val_loss: 4318.7900 - val_accuracy: 0.5000\n",
            "Epoch 4/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1677.2562 - accuracy: 0.3667\n",
            "Epoch 4: val_loss improved from 4318.79004 to 965.79816, saving model to /content/Test_CNN/Models/Test_1/1_cnn.h5\n",
            "45/45 [==============================] - 15s 334ms/step - loss: 1677.2562 - accuracy: 0.3667 - val_loss: 965.7982 - val_accuracy: 0.3000\n",
            "Epoch 5/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 3062334208.0000 - accuracy: 0.3444\n",
            "Epoch 5: val_loss did not improve from 965.79816\n",
            "45/45 [==============================] - 3s 63ms/step - loss: 3062334208.0000 - accuracy: 0.3444 - val_loss: 165223408.0000 - val_accuracy: 0.5000\n",
            "Epoch 6/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 485361568.0000 - accuracy: 0.4444\n",
            "Epoch 6: val_loss did not improve from 965.79816\n",
            "45/45 [==============================] - 3s 63ms/step - loss: 485361568.0000 - accuracy: 0.4444 - val_loss: 1251511.1250 - val_accuracy: 0.5000\n",
            "Epoch 6: early stopping\n",
            "4/4 [==============================] - 6s 418ms/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171446536/171446536 [==============================] - 1s 0us/step\n",
            "Epoch 1/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 31.0110 - accuracy: 0.4000\n",
            "Epoch 1: val_loss improved from inf to 280054081270054912.00000, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 105s 497ms/step - loss: 31.0110 - accuracy: 0.4000 - val_loss: 280054081270054912.0000 - val_accuracy: 0.3000\n",
            "Epoch 2/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 5.6526 - accuracy: 0.3333\n",
            "Epoch 2: val_loss improved from 280054081270054912.00000 to 636775563264.00000, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 19s 642ms/step - loss: 5.6526 - accuracy: 0.3333 - val_loss: 636775563264.0000 - val_accuracy: 0.2000\n",
            "Epoch 3/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.8281 - accuracy: 0.4000\n",
            "Epoch 3: val_loss improved from 636775563264.00000 to 402170.68750, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 20s 683ms/step - loss: 1.8281 - accuracy: 0.4000 - val_loss: 402170.6875 - val_accuracy: 0.3000\n",
            "Epoch 4/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.6283 - accuracy: 0.4111\n",
            "Epoch 4: val_loss improved from 402170.68750 to 1.27427, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 21s 702ms/step - loss: 1.6283 - accuracy: 0.4111 - val_loss: 1.2743 - val_accuracy: 0.5000\n",
            "Epoch 5/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.8055 - accuracy: 0.4556\n",
            "Epoch 5: val_loss improved from 1.27427 to 1.08632, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 25s 842ms/step - loss: 1.8055 - accuracy: 0.4556 - val_loss: 1.0863 - val_accuracy: 0.5000\n",
            "Epoch 6/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.2616 - accuracy: 0.3778\n",
            "Epoch 6: val_loss improved from 1.08632 to 1.08469, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 19s 662ms/step - loss: 1.2616 - accuracy: 0.3778 - val_loss: 1.0847 - val_accuracy: 0.5000\n",
            "Epoch 7/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.3783 - accuracy: 0.4667\n",
            "Epoch 7: val_loss did not improve from 1.08469\n",
            "30/30 [==============================] - 3s 107ms/step - loss: 1.3783 - accuracy: 0.4667 - val_loss: 1.0856 - val_accuracy: 0.5000\n",
            "Epoch 8/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.9789 - accuracy: 0.5667\n",
            "Epoch 8: val_loss improved from 1.08469 to 1.08318, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 14s 468ms/step - loss: 0.9789 - accuracy: 0.5667 - val_loss: 1.0832 - val_accuracy: 0.5000\n",
            "Epoch 9/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.1750 - accuracy: 0.5667\n",
            "Epoch 9: val_loss improved from 1.08318 to 1.07970, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 25s 862ms/step - loss: 1.1750 - accuracy: 0.5667 - val_loss: 1.0797 - val_accuracy: 0.5000\n",
            "Epoch 10/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.1161 - accuracy: 0.5333\n",
            "Epoch 10: val_loss improved from 1.07970 to 1.07673, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 24s 828ms/step - loss: 1.1161 - accuracy: 0.5333 - val_loss: 1.0767 - val_accuracy: 0.5000\n",
            "Epoch 11/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0096 - accuracy: 0.5556\n",
            "Epoch 11: val_loss improved from 1.07673 to 1.07550, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 24s 814ms/step - loss: 1.0096 - accuracy: 0.5556 - val_loss: 1.0755 - val_accuracy: 0.5000\n",
            "Epoch 12/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0077 - accuracy: 0.6000\n",
            "Epoch 12: val_loss improved from 1.07550 to 1.07390, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 20s 675ms/step - loss: 1.0077 - accuracy: 0.6000 - val_loss: 1.0739 - val_accuracy: 0.5000\n",
            "Epoch 13/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0528 - accuracy: 0.5444\n",
            "Epoch 13: val_loss improved from 1.07390 to 1.07239, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 13s 435ms/step - loss: 1.0528 - accuracy: 0.5444 - val_loss: 1.0724 - val_accuracy: 0.5000\n",
            "Epoch 14/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.2181 - accuracy: 0.4556\n",
            "Epoch 14: val_loss improved from 1.07239 to 1.06976, saving model to /content/Test_CNN/Models/Test_1/2_cnn.h5\n",
            "30/30 [==============================] - 17s 587ms/step - loss: 1.2181 - accuracy: 0.4556 - val_loss: 1.0698 - val_accuracy: 0.5000\n",
            "Epoch 15/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 1.0359 - accuracy: 0.5222\n",
            "Epoch 15: val_loss did not improve from 1.06976\n",
            "30/30 [==============================] - 3s 111ms/step - loss: 1.0359 - accuracy: 0.5222 - val_loss: 1.0713 - val_accuracy: 0.5000\n",
            "4/4 [==============================] - 2s 167ms/step\n",
            "Epoch 1/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 11.7314 - accuracy: 0.3444\n",
            "Epoch 1: val_loss improved from inf to 274190.00000, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 110s 388ms/step - loss: 11.7314 - accuracy: 0.3444 - val_loss: 274190.0000 - val_accuracy: 0.2000\n",
            "Epoch 2/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 3.5830 - accuracy: 0.4222\n",
            "Epoch 2: val_loss did not improve from 274190.00000\n",
            "45/45 [==============================] - 7s 148ms/step - loss: 3.5830 - accuracy: 0.4222 - val_loss: 1028254.0000 - val_accuracy: 0.5000\n",
            "Epoch 3/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.9572 - accuracy: 0.3556\n",
            "Epoch 3: val_loss improved from 274190.00000 to 25986.51562, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 16s 370ms/step - loss: 1.9572 - accuracy: 0.3556 - val_loss: 25986.5156 - val_accuracy: 0.5000\n",
            "Epoch 4/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.4610 - accuracy: 0.3556\n",
            "Epoch 4: val_loss improved from 25986.51562 to 708.68280, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 20s 449ms/step - loss: 1.4610 - accuracy: 0.3556 - val_loss: 708.6828 - val_accuracy: 0.2000\n",
            "Epoch 5/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.7028 - accuracy: 0.4000\n",
            "Epoch 5: val_loss improved from 708.68280 to 69.84916, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 24s 551ms/step - loss: 1.7028 - accuracy: 0.4000 - val_loss: 69.8492 - val_accuracy: 0.5000\n",
            "Epoch 6/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.3670 - accuracy: 0.4333\n",
            "Epoch 6: val_loss improved from 69.84916 to 1.09210, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 15s 338ms/step - loss: 1.3670 - accuracy: 0.4333 - val_loss: 1.0921 - val_accuracy: 0.5000\n",
            "Epoch 7/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.6932 - accuracy: 0.3556\n",
            "Epoch 7: val_loss improved from 1.09210 to 1.08805, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 20s 447ms/step - loss: 1.6932 - accuracy: 0.3556 - val_loss: 1.0880 - val_accuracy: 0.5000\n",
            "Epoch 8/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.3916 - accuracy: 0.3778\n",
            "Epoch 8: val_loss improved from 1.08805 to 1.08600, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 20s 451ms/step - loss: 1.3916 - accuracy: 0.3778 - val_loss: 1.0860 - val_accuracy: 0.5000\n",
            "Epoch 9/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.6323 - accuracy: 0.3222\n",
            "Epoch 9: val_loss improved from 1.08600 to 1.08491, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 15s 340ms/step - loss: 1.6323 - accuracy: 0.3222 - val_loss: 1.0849 - val_accuracy: 0.5000\n",
            "Epoch 10/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.2844 - accuracy: 0.4111\n",
            "Epoch 10: val_loss improved from 1.08491 to 1.08319, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 20s 454ms/step - loss: 1.2844 - accuracy: 0.4111 - val_loss: 1.0832 - val_accuracy: 0.5000\n",
            "Epoch 11/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.6805 - accuracy: 0.5222\n",
            "Epoch 11: val_loss improved from 1.08319 to 1.08146, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 24s 545ms/step - loss: 1.6805 - accuracy: 0.5222 - val_loss: 1.0815 - val_accuracy: 0.5000\n",
            "Epoch 12/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.5400 - accuracy: 0.4556\n",
            "Epoch 12: val_loss did not improve from 1.08146\n",
            "45/45 [==============================] - 4s 90ms/step - loss: 1.5400 - accuracy: 0.4556 - val_loss: 1.0822 - val_accuracy: 0.5000\n",
            "Epoch 13/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.5244 - accuracy: 0.4889\n",
            "Epoch 13: val_loss improved from 1.08146 to 1.08012, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 16s 360ms/step - loss: 1.5244 - accuracy: 0.4889 - val_loss: 1.0801 - val_accuracy: 0.5000\n",
            "Epoch 14/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.4346 - accuracy: 0.5000\n",
            "Epoch 14: val_loss improved from 1.08012 to 1.07669, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 23s 513ms/step - loss: 1.4346 - accuracy: 0.5000 - val_loss: 1.0767 - val_accuracy: 0.5000\n",
            "Epoch 15/15\n",
            "45/45 [==============================] - ETA: 0s - loss: 1.7004 - accuracy: 0.4778\n",
            "Epoch 15: val_loss improved from 1.07669 to 1.07449, saving model to /content/Test_CNN/Models/Test_1/3_cnn.h5\n",
            "45/45 [==============================] - 11s 252ms/step - loss: 1.7004 - accuracy: 0.4778 - val_loss: 1.0745 - val_accuracy: 0.5000\n",
            "4/4 [==============================] - 3s 173ms/step\n",
            "Epoch 1/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 2.2412 - accuracy: 0.5667\n",
            "Epoch 1: val_loss improved from inf to 2.40263, saving model to /content/Test_CNN/Models/Test_1/4_cnn.h5\n",
            "30/30 [==============================] - 59s 671ms/step - loss: 2.2412 - accuracy: 0.5667 - val_loss: 2.4026 - val_accuracy: 0.5000\n",
            "Epoch 2/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.7485 - accuracy: 0.8444\n",
            "Epoch 2: val_loss did not improve from 2.40263\n",
            "30/30 [==============================] - 2s 69ms/step - loss: 0.7485 - accuracy: 0.8444 - val_loss: 3.3897 - val_accuracy: 0.2000\n",
            "Epoch 3/15\n",
            "30/30 [==============================] - ETA: 0s - loss: 0.2150 - accuracy: 0.9111\n",
            "Epoch 3: val_loss did not improve from 2.40263\n",
            "30/30 [==============================] - 2s 69ms/step - loss: 0.2150 - accuracy: 0.9111 - val_loss: 9.4637 - val_accuracy: 0.2000\n",
            "Epoch 3: early stopping\n",
            "4/4 [==============================] - 1s 111ms/step\n"
          ]
        }
      ],
      "source": [
        "val_path_dataset = '/content/Test_CNN/'\n",
        "os.chdir(val_path_dataset)\n",
        "\n",
        "if not Path(dir_save).is_dir():\n",
        "  os.mkdir(dir_save)\n",
        "\n",
        "print('\\n*** starting at',pd.Timestamp.now())\n",
        "start_time_total = time.time()\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(directions=optimizer_direction,study_name=\"starter-experiment\")\n",
        "study.optimize(objective, n_trials=5,gc_after_trial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbX03qiOGgV",
        "outputId": "57de4253-e753-4599-af1f-70631b6bee32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "total elapsed time = 19.304691219329833  minutes\n"
          ]
        }
      ],
      "source": [
        "# save results\n",
        "df_results = study.trials_dataframe()\n",
        "if not Path(results_dir).is_dir():\n",
        "  os.mkdir(results_dir)\n",
        "df_results.to_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "df_results.to_csv(results_dir + 'df_optuna_results.csv')\n",
        "elapsed_time_total = (time.time()-start_time_total)/60\n",
        "print('\\n\\ntotal elapsed time =',elapsed_time_total,' minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN_1Z9uNjmk"
      },
      "source": [
        "# Sort results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO7C-8OZNivN",
        "outputId": "4b30c837-c942-4f96-92b4-eb91f5864110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted CSV file (according to multiple columns) =     Unnamed: 0  number    values_0  values_1              datetime_start  \\\n",
            "0           0       0         NaN       NaN  2023-07-31 13:54:18.691758   \n",
            "2           2       2    1.069757       0.5  2023-07-31 13:59:54.031920   \n",
            "3           3       3    1.074495       0.5  2023-07-31 14:05:58.178749   \n",
            "4           4       4    2.402635       0.5  2023-07-31 14:11:54.931528   \n",
            "1           1       1  965.798157       0.5  2023-07-31 13:58:58.287692   \n",
            "\n",
            "            datetime_complete                duration  params_batch_size  \\\n",
            "0  2023-07-31 13:58:56.803408  0 days 00:04:38.111650                  3   \n",
            "2  2023-07-31 14:05:57.191551  0 days 00:06:03.159631                  3   \n",
            "3  2023-07-31 14:11:53.618867  0 days 00:05:55.440118                  2   \n",
            "4  2023-07-31 14:13:36.128916  0 days 00:01:41.197388                  3   \n",
            "1  2023-07-31 13:59:53.679613  0 days 00:00:55.391921                  2   \n",
            "\n",
            "  params_cnn_model  params_dense_nodes_divisor  params_drop_out_rate  \\\n",
            "0        resnet152                           2                   0.4   \n",
            "2        resnet101                           2                   0.1   \n",
            "3        resnet101                           8                   0.4   \n",
            "4         resnet50                           8                   0.0   \n",
            "1            VGG19                           4                   0.1   \n",
            "\n",
            "   params_lr  params_num_dense_nodes  params_num_layers  \\\n",
            "0     0.0100                     256                  2   \n",
            "2     0.0010                     512                  1   \n",
            "3     0.0010                     256                  2   \n",
            "4     0.0001                     512                  2   \n",
            "1     0.0100                     512                  1   \n",
            "\n",
            "   system_attrs_nsga2:generation     state  \n",
            "0                              0      FAIL  \n",
            "2                              0  COMPLETE  \n",
            "3                              0  COMPLETE  \n",
            "4                              0  COMPLETE  \n",
            "1                              0  COMPLETE  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "val_path_dataset = '/content/Test_CNN/Results_optuna/Test_'+teste_number#path where optuna dataframe was saved\n",
        "os.chdir(val_path_dataset)\n",
        "dataframe = pd.read_csv('df_optuna_results.csv')\n",
        "dataframe.sort_values(\"values_0\",axis=0,ascending=True, inplace=True, na_position='first')\n",
        "print(\"Sorted CSV file (according to multiple columns) = \", dataframe.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnDo77oT1MI"
      },
      "source": [
        "# Evaluate best models in split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FR0xpKTW1M0"
      },
      "source": [
        "Test with fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qT9tHTTKQRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ca25ea-acad-43ee-a185-5104f3b69546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 101ms/step\n"
          ]
        }
      ],
      "source": [
        "best_model = keras.models.load_model('/content/Test_CNN/Models/Test_'+teste_number+'0_cnn.h5')\n",
        "prob_class = best_model.predict(x_test_fundus)#probabilities\n",
        "predicted_classes = np.argmax(prob_class, axis=1)\n",
        "trial_number = 99\n",
        "gera_csv(predicted_classes, classification_results_dir,str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA0tFXlBC2hF"
      },
      "source": [
        "# Ensemble of best models from optimization process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "na1sSgTzDN3K",
        "outputId": "8a8b4488-dac7-4297-bcf5-b8b0c7f4ee1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "4/4 [==============================] - 3s 178ms/step\n",
            "3\n",
            "4/4 [==============================] - 3s 173ms/step\n",
            "4\n",
            "4/4 [==============================] - 2s 115ms/step\n"
          ]
        }
      ],
      "source": [
        "def make_final_predictions(xprod,\n",
        "                           models_directory,\n",
        "                           df_params,\n",
        "                           threshold, num_models_accept,\n",
        "                           optimization_direction):\n",
        "\n",
        "    if optimization_direction == 'maximize':\n",
        "        df_params.sort_values(by='values_1', ascending=False, inplace=True)\n",
        "    else:\n",
        "        df_params.sort_values(by='values_0', ascending=True, inplace=True)\n",
        "\n",
        "    # apply threshold\n",
        "    accepted_models_num = 0\n",
        "    list_predicted_prob = []\n",
        "    num_models_total = df_params.shape[0]\n",
        "    for i in range(num_models_total):\n",
        "        if optimization_direction == 'maximize':\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_1'] > threshold\n",
        "        else:\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_0'] < threshold\n",
        "\n",
        "        bool2 = df_params.loc[df_params.index[i],'state'] == 'COMPLETE'\n",
        "        bool3 = accepted_models_num < num_models_accept\n",
        "        if bool1 and bool2 and bool3:\n",
        "            model_number = str(df_params.loc[df_params.index[i],'number'])\n",
        "            #if model_number != '6':\n",
        "            print (model_number)\n",
        "            try:\n",
        "                cnn_model = keras.models.load_model(models_directory + model_number + '_cnn.h5')\n",
        "            except:\n",
        "                print('\\ncould not read model number:',model_number)\n",
        "            else:\n",
        "                list_predicted_prob.append(cnn_model.predict(xprod))\n",
        "                accepted_models_num = accepted_models_num + 1\n",
        "\n",
        "    # compute mean probabilities\n",
        "    mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "    # compute predicted class\n",
        "    # argmax uses 1st ocurrance in case of a tie\n",
        "    y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "    return y_predicted_class\n",
        "\n",
        "# fixed parameters - production (Ensemble)\n",
        "threshold_error =  3# validation loss\n",
        "number_of_models = 5\n",
        "\n",
        "# get optuna results parameters\n",
        "#models_dir = results_directory_stub + 'calibration/'\n",
        "df_parameters = pd.read_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "\n",
        "#results_directory = results_directory_stub + calculation_type + '/'\n",
        "#if not Path(results_directory).is_dir():\n",
        "    #os.mkdir(results_directory)\n",
        "\n",
        "final_result = make_final_predictions(x_test_fundus,\n",
        "                               dir_save,\n",
        "                               df_parameters,\n",
        "                               threshold_error,\n",
        "                               number_of_models, optimizer_direction)\n",
        "\n",
        "trial_number = 100\n",
        "gera_csv(final_result, classification_results_dir, str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZi9_pW4-Lhq"
      },
      "source": [
        "# Combine best Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJKhvg6pahGI"
      },
      "outputs": [],
      "source": [
        "#Used in ensemble\n",
        "x_test_list=[]\n",
        "#Images\n",
        "x_test_list.append(x_test_fundus)\n",
        "x_test_list.append(x_test_ROI)\n",
        "x_test_list.append(X_test_OCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnlJXEbqZZGO"
      },
      "outputs": [],
      "source": [
        "def ensemble_results(cnn_model,x):\n",
        "    return cnn_model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqzkcPN2-Wr3"
      },
      "outputs": [],
      "source": [
        "#best load model with background image as input\n",
        "#best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_1/22_cnn.h5')#VGG19\n",
        "models_list = []\n",
        "best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Teste_11/32_cnn.h5')#Dense169 - 224x224\n",
        "models_list.append(best_model_fundus)\n",
        "#load best model with ROI as input\n",
        "best_model_ROI = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_2A/16_cnn.h5')#VGG19 - 128x128\n",
        "models_list.append(best_model_ROI)\n",
        "#load better model with OCT as input\n",
        "best_model_OCT = keras.models.load_model('Modelos_CNN_Optuna/Testes_30_Trials/Teste_3/11_cnn.h5')#Dense121 - 3D\n",
        "models_list.append(best_model_OCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE78lHDQZ44J",
        "outputId": "431fcb66-2fde-4bf1-de81-d51c3cca9298"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 14s 2s/step\n",
            "4/4 [==============================] - 12s 3s/step\n",
            "4/4 [==============================] - 126s 27s/step\n"
          ]
        }
      ],
      "source": [
        "list_predicted_prob = []\n",
        "for i in range (len(models_list)):\n",
        "  list_predicted_prob.append(ensemble_results(models_list[i], x_test_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRJRNcZdf4J"
      },
      "source": [
        "Average Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HluJZeOAdhjD"
      },
      "outputs": [],
      "source": [
        "# compute mean probabilities\n",
        "mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "# compute predicted class\n",
        "# argmax uses 1st ocurrance in case of a tie\n",
        "y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "#gera_csv\n",
        "gera_csv(y_predicted_class, classification_results_dir,str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mode ensemble"
      ],
      "metadata": {
        "id": "PGoTSwDs6CW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## function to identify cases where 2 models predicted in a class\n",
        "def conta_votos_2x1(v1,v2,v3):\n",
        "  votos = []\n",
        "  # 1, 2 ou 3\n",
        "  index1 = v1.index('1')#model fundus stage predicition\n",
        "  votos.append(index1)\n",
        "  index2 = v2.index('1')#model roi stage predicition\n",
        "  votos.append(index2)\n",
        "  index3 = v3.index('1')#model oct stage predicition\n",
        "  votos.append(index3)\n",
        "\n",
        "  #verifica quem teve 2 votos\n",
        "  if votos.count(1) == 2:\n",
        "    a=[v1[0],'1','0','0']\n",
        "    return (a)\n",
        "  if votos.count(2) == 2:\n",
        "    a=[v1[0],'0','1','0']\n",
        "    return (a)\n",
        "  if votos.count(3) == 2:\n",
        "    a=[v1[0],'0','0','1']\n",
        "    return (a)"
      ],
      "metadata": {
        "id": "9NySfjkgzWaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function gives the following priority to resolve conflicts\n",
        "# if the oct models gives an initial diagnosis result, it will be kept\n",
        "# if the fundus model is advanced or normal it will be kept\n",
        "# otherwise the fundus model result will be kept\n",
        "def analisa_conflito(v1,v2,v3):\n",
        "  votos = []\n",
        "  index1 = v1.index('1')\n",
        "  votos.append(index1)\n",
        "  index2 = v2.index('1')\n",
        "  votos.append(index2)\n",
        "  index3 = v3.index('1')\n",
        "  votos.append(index3)\n",
        "\n",
        "  if index3 == 2: #result of oct model is early glaucoma\n",
        "    return ([v1[0],'0','1','0'])\n",
        "  else:\n",
        "    return v1"
      ],
      "metadata": {
        "id": "YSeQ_dWm0Wpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pprint\n",
        "\n",
        "#extract the data from the csv file and put it in a list\n",
        "fundus_predictions = []\n",
        "roi_predictions = []\n",
        "oct_predictions = []\n",
        "#ensemble = []\n",
        "#csv of best models\n",
        "results_fundo = os.path.join(path_dataset, 'Melhores resultados/Classification_Results_3_Trial_32.csv')#classification of fundus model\n",
        "results_roi = os.path.join(path_dataset, 'Melhores resultados/Classification_Results_3_Trial_16.csv')#classification of roi model\n",
        "results_oct = os.path.join(path_dataset, 'Melhores resultados/Classification_Results_2_Trial_11.csv')#classification of oct model\n",
        "\n",
        "with open(results_fundo,'r') as f:\n",
        "    lines=csv.reader(f)\n",
        "    for key, line in enumerate(lines):\n",
        "        fundus_predictions.append(line)\n",
        "\n",
        "with open(results_roi,'r') as f:\n",
        "    lines=csv.reader(f)\n",
        "    for key, line in enumerate(lines):\n",
        "        roi_predictions.append(line)\n",
        "\n",
        "with open(results_oct,'r') as f:\n",
        "    lines=csv.reader(f)\n",
        "    for key, line in enumerate(lines):\n",
        "        oct_predictions.append(line)"
      ],
      "metadata": {
        "id": "3ST3H70STHRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = []\n",
        "for i in range(len(fundus_predictions)):\n",
        "  if i == 0:\n",
        "    ensemble.append(fundus_predictions[i])#Name of each column\n",
        "  elif fundus_predictions[i] == roi_predictions[i] and roi_predictions[i] == oct_predictions[i]:#All models predict same class\n",
        "    ensemble.append(fundus_predictions[i])\n",
        "  elif fundus_predictions[i] != roi_predictions[i] and roi_predictions[i] != oct_predictions[i] and fundus_predictions[i] != oct_predictions[i]:#each model predicts a different class\n",
        "    ensemble.append(fundus_predictions[i])#keeps fundus model result\n",
        "    #ensemble.append(analisa_conflito(fundus_predictions[i], roi_predictions[i], oct_predictions[i]))#uses function to determine final result\n",
        "  else:\n",
        "    ensemble.append(conta_votos_2x1(fundus_predictions[i], roi_predictions[i], oct_predictions[i]))#checks cases where 2 models predicted the same class\n",
        "\n",
        "#create a csv\n",
        "results_file_path = os.path.join(val_path_dataset, \"./Classification Results Optuna/Ensemble/Classification_Results.csv\")\n",
        "with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "  writer = csv.writer(f)\n",
        "  #writer.writerow(header)\n",
        "  writer.writerows(ensemble)"
      ],
      "metadata": {
        "id": "Tv0-Fviu0aqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4odujajzijHe",
        "qPN2YsxZmki0",
        "dP4xlwCWNFfn",
        "wKUezmqObFkP",
        "OjM_QudD7nP9",
        "uA0tFXlBC2hF",
        "QFkaOH2XWw69",
        "W4R9n8EEXRcq",
        "sC_a8OAJu7We",
        "9fmQ-6Rv2qTD",
        "C0HE_wEfLkWD",
        "gmuOfFOAf03E",
        "9PoS0_oRjGRZ",
        "BK4BQEDOMIyk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}