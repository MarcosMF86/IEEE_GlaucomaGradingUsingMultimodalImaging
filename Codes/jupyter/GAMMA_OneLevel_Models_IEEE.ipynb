{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rx06mo_SI5sT"
      },
      "source": [
        "# Path to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duh7fkJVIFMJ",
        "outputId": "c04e176b-f24e-4378-b78f-18a1662da6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_training_data/multi-modality_images/'#path to dataset\n",
        "os.chdir(path_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFI8Ds2VJMvB"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGTsyeK7a6nT"
      },
      "outputs": [],
      "source": [
        "pip install -U tensorflow-addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xkg1_ngQTDcx"
      },
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHwqbK7PSUDG"
      },
      "source": [
        "# 3D CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mylq9tzuSX4O"
      },
      "outputs": [],
      "source": [
        "!pip install classification-models-3D\n",
        "!pip install keras_applications\n",
        "from classification_models_3D.tfkeras import Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3A4VA8dv05n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import glob, warnings\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "warnings.filterwarnings('ignore')\n",
        "print('TensorFlow Version ' + tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9aGk1GntJPWa"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "from io import BytesIO\n",
        "from keras.layers import Dense, Activation, Flatten, Dropout, GlobalAveragePooling2D, Concatenate, Reshape, GlobalMaxPooling2D, GlobalMaxPooling3D, GlobalAveragePooling3D, Conv2D, Conv1D, Add\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "import sklearn\n",
        "from sklearn.metrics import cohen_kappa_score, roc_auc_score, roc_curve,accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import random\n",
        "import cv2\n",
        "import gdown\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow import keras\n",
        "import random\n",
        "import sklearn\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "from tensorflow.keras import layers\n",
        "import csv\n",
        "from utils_GAMMA_V2 import padroniza_resultado\n",
        "from utils_GAMMA_V2 import converte\n",
        "from scipy import stats\n",
        "from keras.applications.densenet import DenseNet121\n",
        "from keras.applications.densenet import DenseNet169\n",
        "from keras.applications.resnet import ResNet50\n",
        "from keras.applications.resnet import ResNet101\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.models import Sequential, Model\n",
        "import time\n",
        "from pathlib import Path\n",
        "import gc\n",
        "from keras.backend import int_shape\n",
        "import pickle\n",
        "import optuna\n",
        "from keras.applications.densenet import preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_LCC90hOA38"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "x1Dzcr6_OEWH"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = 224\n",
        "depth = 64\n",
        "# TRAINING\n",
        "EPOCHS = 15\n",
        "early_stop_epochs = 2\n",
        "learning_rate_epochs = 5\n",
        "optimizer_direction = ['minimize', \"maximize\"]\n",
        "#Foldes to save models and dataframes results\n",
        "teste_number = '1/'\n",
        "dir_save = '/content/Test_CNN/Models/Test_' + teste_number\n",
        "results_dir = '/content/Test_CNN/Results_optuna/Test_' + teste_number\n",
        "classification_results_dir = '/content/Test_CNN/Classification Results Optuna/Test_' + teste_number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nChTJ8nZIeaE"
      },
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Qszb6AnZIeu6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "145dc3b3-2174-4261-a3e2-eea6540d342b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "folder_fundus = 'dataset_fundus/train/fundus_images flip - 100 images/'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "\n",
        "conj = 0# 0--> load train images; 1--> load test images\n",
        "X_train_fundus,X_train_ROI,Y = load_fundus_images(path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)\n",
        "num_classes = len(np.unique(Y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sH0qrRfCK2H"
      },
      "source": [
        "Read OCTs Volumes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Rl69maz-CKJB"
      },
      "outputs": [],
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "scan_paths = dir_octs(path_dataset,'multi-modality_images',0)#path to 3D volumes\n",
        "X_oct = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read 3D volumes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OfWV4riR5cOX"
      },
      "source": [
        "Split Train/Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fundus"
      ],
      "metadata": {
        "id": "bHzmMo2Pwner"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJaA-9Q65feN",
        "outputId": "d6a1c7e2-577e-4662-b879-2c6bf9212367"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_fundus, x_val_fundus, y_train, y_val = train_test_split(X_train_fundus, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_fundus.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_fundus.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdt6hjh47CB2"
      },
      "source": [
        "ROIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRcvKYwe6t8o",
        "outputId": "dafc1ee8-812c-44cb-b157-04472f734fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (90, 224, 224, 3) - y_train shape: (90,)\n",
            "x_val shape: (10, 224, 224, 3) - y_val shape: (10,)\n"
          ]
        }
      ],
      "source": [
        "x_train_ROI, x_val_ROI, y_train, y_val = train_test_split(X_train_ROI, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_ROI.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_ROI.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXteIGLv7ExN"
      },
      "source": [
        "OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqdcH2sue_mY"
      },
      "outputs": [],
      "source": [
        "x_train_oct, x_val_oct, y_train, y_val = train_test_split(X_oct, Y, test_size=0.1, random_state=42,stratify=Y)\n",
        "\n",
        "print(f\"x_train shape: {x_train_oct.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_val shape: {x_val_oct.shape} - y_val shape: {y_val.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read Fundus Images and Optic Disc Images (ROIs) - Test"
      ],
      "metadata": {
        "id": "70gIQIWCyuC7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-h-gFAFTTov",
        "outputId": "493c750b-89dc-4468-a00c-8a172125b94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "x_teste shape: (100, 224, 224, 3)\n"
          ]
        }
      ],
      "source": [
        "from utils_GAMMA_V2 import load_fundus_images\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "folder_fundus = 'fundus_images'\n",
        "folder_fundus_2 = 'ROI_disco_50px'\n",
        "conj = 1# 1-->Load Images from test set\n",
        "x_test_fundus,x_test_ROI = load_fundus_images(val_path_dataset,folder_fundus,folder_fundus_2,conj,IMAGE_SIZE)#\n",
        "print(f\"x_teste shape: {x_test_fundus.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read OCT Volumes"
      ],
      "metadata": {
        "id": "boD-pbzx0mKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils_GAMMA_V2 import process_scan\n",
        "from utils_GAMMA_V2 import dir_octs\n",
        "\n",
        "val_path_dataset = '/content/gdrive/My Drive/Doutorado/Bases/GAMMA_val_data/multi-modality_images/'\n",
        "os.chdir(val_path_dataset)\n",
        "conj = 1#conj = 0 (Train); conj = 1(Test)\n",
        "scan_paths = dir_octs(val_path_dataset,'multi-modality_images',conj)\n",
        "#scan_paths = scan_paths[:1]\n",
        "X_test_OCT = np.array([process_scan(path,IMAGE_SIZE, depth) for path in scan_paths])#read volumes"
      ],
      "metadata": {
        "id": "JP5QL2Kg0lcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dVcEwSxPWRh"
      },
      "source": [
        "# Plot Fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hmDQk3XPYRy"
      },
      "outputs": [],
      "source": [
        " # number of images\n",
        "n_images = 100\n",
        "\n",
        "# arrays\n",
        "x = X_train_fundus\n",
        "y = Y\n",
        "\n",
        "# numer of rows and columns\n",
        "rows = 10\n",
        "cols = 10\n",
        "\n",
        "# empty figure\n",
        "fig, axs = plt.subplots(rows, cols, figsize=(28, 28))\n",
        "axs = axs.ravel()\n",
        "\n",
        "# para cada imagem\n",
        "for i in range(n_images):\n",
        "    #subplot\n",
        "    axs[i].imshow(x[i], cmap='gray')\n",
        "    axs[i].set_title(f'Label: {y[i]}')\n",
        "    # remove as bordas e o eixo x, y\n",
        "    axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate CSV to be evaluated online"
      ],
      "metadata": {
        "id": "4A64nNEz2F3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gera_csv(predicted_classes, classification_results_dir, trial_number):\n",
        "  #classification_results_dir = 'content/Test_CNN/Classification Results Optuna/Test_'+teste_number\n",
        "  if not Path(classification_results_dir).is_dir():\n",
        "    os.mkdir(classification_results_dir)\n",
        "  header = ['data','non','early','mid_advanced']\n",
        "  results = padroniza_resultado(predicted_classes)\n",
        "  #Alterar a pasta\n",
        "  results_file_path = os.path.join(val_path_dataset, classification_results_dir  + '/Classification_Results_'+trial_number+'.csv')\n",
        "  with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    writer.writerows(results)"
      ],
      "metadata": {
        "id": "CFQTdFCY2G-Q"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4m5hBnEsNyn"
      },
      "source": [
        "# Optimize One-level models; Inputs: Fundus or ROI Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0nM2vUymiWO"
      },
      "source": [
        "1 level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c7UcFsLAL392"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [64, 128, 256, 512])\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001,])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['VGG19', 'resnet50', 'resnet101','resnet152','Densenet121','Densenet169'])\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])#fully connected layers\n",
        "  #i = tf.keras.layers.Input([IMAGE_SIZE,IMAGE_SIZE, 3], dtype = tf.uint8)\n",
        "  #x = tf.cast(i, tf.float32)\n",
        "\n",
        "  if cnn_model == 'InceptionV3':\n",
        "    model_fundus = InceptionV3(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'VGG19':\n",
        "    model_fundus = VGG19(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    model_fundus = ResNet50(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    model_fundus = ResNet101(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    model_fundus = ResNet152(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    model_fundus = DenseNet121(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    model_fundus = DenseNet169(weights='imagenet',\n",
        "                        include_top=False,\n",
        "                        input_shape=(IMAGE_SIZE,IMAGE_SIZE, 3))\n",
        "\n",
        "\n",
        "  x = model_fundus.output\n",
        "  x = Flatten()(x)\n",
        "  #MLP\n",
        "  #x = Dense(num_dense_nodes, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  if num_layers == 2:\n",
        "    #x = Dense(num_dense_nodes//dense_nodes_divisor, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "    x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "    x = Dropout(drop_out_rate)(x)\n",
        "\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instantiate and compile model\n",
        "  model = Model(inputs=model_fundus.input,outputs=output_tensor)\n",
        "\n",
        "  #model = tf.keras.Model(inputs=[i], outputs=[output_tensor])\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping, learning rate reducer and model checkpoint\n",
        "  fn = dir_save + str(trial.number) + '_cnn.h5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, mode='auto', min_delta=1e-3,verbose=1),\n",
        "                     #MyThresholdCallback(threshold=0.95),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=1, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss', verbose=1,save_best_only=True),\n",
        "                    #PlotLossesKeras()\n",
        "                    ]\n",
        "\n",
        "  # fit the model (x_fundus or x_ROI)\n",
        "  h = model.fit(x=x_train_ROI, y=y_train,\n",
        "                #x=X_train_fundus, y = Y,#\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = (x_val_ROI,y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4odujajzijHe"
      },
      "source": [
        "# Optimize One-level models; Inputs: 3D OCTs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W48lSRb2inFX"
      },
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "  num_dense_nodes = trial.suggest_categorical('num_dense_nodes', [32, 64, 128, 256])\n",
        "  dense_nodes_divisor = trial.suggest_categorical('dense_nodes_divisor',[2, 4, 8])\n",
        "  lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
        "  batch_size = trial.suggest_categorical('batch_size', [1, 2, 3])\n",
        "  drop_out_rate=trial.suggest_float('drop_out_rate', 0.0, 0.5, step=0.1)\n",
        "  num_layers = trial.suggest_categorical('num_layers',[1,2])\n",
        "  cnn_model = trial.suggest_categorical('cnn_model', ['VGG19', 'resnet50', 'resnet101','resnet152','Densenet121','Densenet169'])\n",
        "  canais = 3\n",
        "\n",
        "  if cnn_model == 'InceptionV3':\n",
        "    Rede3D, preprocess_input = Classifiers.get('inceptionv3')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'VGG19':\n",
        "    Rede3D, preprocess_input = Classifiers.get('vgg19')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet50':\n",
        "    Rede3D, preprocess_input = Classifiers.get('resnet50')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet101':\n",
        "    Rede3D, preprocess_input = Classifiers.get('resnet101')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'resnet152':\n",
        "    Rede3D, preprocess_input = Classifiers.get('resnet152')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'Densenet121':\n",
        "    Rede3D, preprocess_input = Classifiers.get('densenet121')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  if cnn_model == 'Densenet169':\n",
        "    Rede3D, preprocess_input = Classifiers.get('densenet169')\n",
        "    model_oct = Rede3D(input_shape=(depth, IMAGE_SIZE, IMAGE_SIZE, canais), weights='imagenet',include_top=False)\n",
        "\n",
        "  x = model_oct.output\n",
        "  x = Flatten()(x)\n",
        "  #MLP\n",
        "  #x = Dense(num_dense_nodes, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4), activation='relu')(x)\n",
        "  x = Dense(num_dense_nodes, activation='relu')(x)\n",
        "  x = Dropout(drop_out_rate)(x)\n",
        "  if num_layers == 2:\n",
        "    #x = Dense(num_dense_nodes//dense_nodes_divisor, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),activation='relu')(x)\n",
        "    x = Dense(num_dense_nodes//dense_nodes_divisor, activation='relu')(x)\n",
        "    x = Dropout(drop_out_rate)(x)\n",
        "  output_tensor = Dense(num_classes, activation='softmax')(x)\n",
        "  #Instantiate and compile model\n",
        "  model = Model(inputs=model_oct.input,outputs=output_tensor)\n",
        "  opt = Adam(learning_rate=lr)  # default = 0.001 // learning_rate=lr\n",
        "  model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])#loss='sparse_categorical_crossentropy'\n",
        "\n",
        "  # callbacks for early stopping and for learning rate reducer\n",
        "  fn = dir_save + str(trial.number) + '_cnn.h5'\n",
        "  callbacks_list = [EarlyStopping(monitor='val_loss', patience=early_stop_epochs, mode='auto',verbose=1),\n",
        "                    #ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=learning_rate_epochs, verbose=1, mode='auto', min_lr=1.0e-6),\n",
        "                    ModelCheckpoint(filepath=fn, monitor='val_loss',verbose=1, save_best_only=True),\n",
        "                    ]\n",
        "\n",
        "  # fit the model\n",
        "  h = model.fit(x=x_train_oct, y=y_train,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=EPOCHS,\n",
        "                          #validation_split=0.1,\n",
        "                          validation_data = (x_val_oct,y_val),\n",
        "                          shuffle=True, verbose=1,\n",
        "                          callbacks=callbacks_list)\n",
        "\n",
        "  validation_loss = np.min(h.history['val_loss'])\n",
        "  val_acc = np.max(h.history['val_accuracy'])\n",
        "  #Evaluate on test set (define a treshold: val loss or acc )\n",
        "  prob_class = model.predict(x_test_fundus)#probabilities\n",
        "  predicted_classes = np.argmax(prob_class, axis=1)\n",
        "  gera_csv(predicted_classes, classification_results_dir, str(trial.number))\n",
        "  return validation_loss, val_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw-Ec_iVizQA"
      },
      "source": [
        "#Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zerlCBMWgNn"
      },
      "outputs": [],
      "source": [
        "val_path_dataset = '/content/Test_CNN/'\n",
        "os.chdir(val_path_dataset)\n",
        "\n",
        "if not Path(dir_save).is_dir():\n",
        "  os.mkdir(dir_save)\n",
        "\n",
        "print('\\n*** starting at',pd.Timestamp.now())\n",
        "start_time_total = time.time()\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "study = optuna.create_study(directions=optimizer_direction,study_name=\"starter-experiment\")\n",
        "study.optimize(objective, n_trials=5,gc_after_trial=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSbX03qiOGgV",
        "outputId": "57de4253-e753-4599-af1f-70631b6bee32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "total elapsed time = 19.304691219329833  minutes\n"
          ]
        }
      ],
      "source": [
        "# save results\n",
        "df_results = study.trials_dataframe()\n",
        "if not Path(results_dir).is_dir():\n",
        "  os.mkdir(results_dir)\n",
        "df_results.to_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "df_results.to_csv(results_dir + 'df_optuna_results.csv')\n",
        "elapsed_time_total = (time.time()-start_time_total)/60\n",
        "print('\\n\\ntotal elapsed time =',elapsed_time_total,' minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptN_1Z9uNjmk"
      },
      "source": [
        "# Sort results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RO7C-8OZNivN",
        "outputId": "4b30c837-c942-4f96-92b4-eb91f5864110"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorted CSV file (according to multiple columns) =     Unnamed: 0  number    values_0  values_1              datetime_start  \\\n",
            "0           0       0         NaN       NaN  2023-07-31 13:54:18.691758   \n",
            "2           2       2    1.069757       0.5  2023-07-31 13:59:54.031920   \n",
            "3           3       3    1.074495       0.5  2023-07-31 14:05:58.178749   \n",
            "4           4       4    2.402635       0.5  2023-07-31 14:11:54.931528   \n",
            "1           1       1  965.798157       0.5  2023-07-31 13:58:58.287692   \n",
            "\n",
            "            datetime_complete                duration  params_batch_size  \\\n",
            "0  2023-07-31 13:58:56.803408  0 days 00:04:38.111650                  3   \n",
            "2  2023-07-31 14:05:57.191551  0 days 00:06:03.159631                  3   \n",
            "3  2023-07-31 14:11:53.618867  0 days 00:05:55.440118                  2   \n",
            "4  2023-07-31 14:13:36.128916  0 days 00:01:41.197388                  3   \n",
            "1  2023-07-31 13:59:53.679613  0 days 00:00:55.391921                  2   \n",
            "\n",
            "  params_cnn_model  params_dense_nodes_divisor  params_drop_out_rate  \\\n",
            "0        resnet152                           2                   0.4   \n",
            "2        resnet101                           2                   0.1   \n",
            "3        resnet101                           8                   0.4   \n",
            "4         resnet50                           8                   0.0   \n",
            "1            VGG19                           4                   0.1   \n",
            "\n",
            "   params_lr  params_num_dense_nodes  params_num_layers  \\\n",
            "0     0.0100                     256                  2   \n",
            "2     0.0010                     512                  1   \n",
            "3     0.0010                     256                  2   \n",
            "4     0.0001                     512                  2   \n",
            "1     0.0100                     512                  1   \n",
            "\n",
            "   system_attrs_nsga2:generation     state  \n",
            "0                              0      FAIL  \n",
            "2                              0  COMPLETE  \n",
            "3                              0  COMPLETE  \n",
            "4                              0  COMPLETE  \n",
            "1                              0  COMPLETE  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "val_path_dataset = '/content/Test_CNN/Results_optuna/Test_'+teste_number#path where optuna dataframe was saved\n",
        "os.chdir(val_path_dataset)\n",
        "dataframe = pd.read_csv('df_optuna_results.csv')\n",
        "dataframe.sort_values(\"values_0\",axis=0,ascending=True, inplace=True, na_position='first')\n",
        "print(\"Sorted CSV file (according to multiple columns) = \", dataframe.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRnDo77oT1MI"
      },
      "source": [
        "# Evaluate best models in split test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FR0xpKTW1M0"
      },
      "source": [
        "Test with fundus Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qT9tHTTKQRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ca25ea-acad-43ee-a185-5104f3b69546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 1s 101ms/step\n"
          ]
        }
      ],
      "source": [
        "best_model = keras.models.load_model('/content/Test_CNN/Models/Test_'+teste_number+'0_cnn.h5')\n",
        "prob_class = best_model.predict(x_test_fundus)#probabilities\n",
        "predicted_classes = np.argmax(prob_class, axis=1)\n",
        "trial_number = 99\n",
        "gera_csv(predicted_classes, classification_results_dir,str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uA0tFXlBC2hF"
      },
      "source": [
        "# Ensemble of best models from optimization process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "na1sSgTzDN3K"
      },
      "outputs": [],
      "source": [
        "def make_final_predictions(xprod,\n",
        "                           models_directory,\n",
        "                           df_params,\n",
        "                           threshold, num_models_accept,\n",
        "                           optimization_direction):\n",
        "\n",
        "    if optimization_direction == 'maximize':\n",
        "        df_params.sort_values(by='values_1', ascending=False, inplace=True)\n",
        "    else:\n",
        "        df_params.sort_values(by='values_0', ascending=True, inplace=True)\n",
        "\n",
        "    # apply threshold\n",
        "    accepted_models_num = 0\n",
        "    list_predicted_prob = []\n",
        "    num_models_total = df_params.shape[0]\n",
        "    for i in range(num_models_total):\n",
        "        if optimization_direction == 'maximize':\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_1'] > threshold\n",
        "        else:\n",
        "            bool1 = df_params.loc[df_params.index[i],'values_0'] < threshold\n",
        "\n",
        "        bool2 = df_params.loc[df_params.index[i],'state'] == 'COMPLETE'\n",
        "        bool3 = accepted_models_num < num_models_accept\n",
        "        if bool1 and bool2 and bool3:\n",
        "            model_number = str(df_params.loc[df_params.index[i],'number'])\n",
        "            #if model_number != '6':\n",
        "            print (model_number)\n",
        "            try:\n",
        "                cnn_model = keras.models.load_model(models_directory + model_number + '_cnn.h5')\n",
        "            except:\n",
        "                print('\\ncould not read model number:',model_number)\n",
        "            else:\n",
        "                list_predicted_prob.append(cnn_model.predict(xprod))\n",
        "                accepted_models_num = accepted_models_num + 1\n",
        "\n",
        "    # compute mean probabilities\n",
        "    mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "    # compute predicted class\n",
        "    # argmax uses 1st ocurrance in case of a tie\n",
        "    y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "    return y_predicted_class\n",
        "\n",
        "# fixed parameters - production (Ensemble)\n",
        "threshold_error =  3# validation loss\n",
        "number_of_models = 5\n",
        "\n",
        "# get optuna results parameters\n",
        "#models_dir = results_directory_stub + 'calibration/'\n",
        "df_parameters = pd.read_pickle(results_dir + 'df_optuna_results.pkl')\n",
        "\n",
        "#results_directory = results_directory_stub + calculation_type + '/'\n",
        "#if not Path(results_directory).is_dir():\n",
        "    #os.mkdir(results_directory)\n",
        "\n",
        "final_result = make_final_predictions(x_test_fundus,\n",
        "                               dir_save,\n",
        "                               df_parameters,\n",
        "                               threshold_error,\n",
        "                               number_of_models, optimizer_direction)\n",
        "\n",
        "trial_number = 100\n",
        "gera_csv(final_result, classification_results_dir, str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZi9_pW4-Lhq"
      },
      "source": [
        "# Combine best Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJKhvg6pahGI"
      },
      "outputs": [],
      "source": [
        "#Used in ensemble\n",
        "x_test_list=[]\n",
        "#Images\n",
        "x_test_list.append(x_test_fundus)\n",
        "x_test_list.append(x_test_ROI)\n",
        "x_test_list.append(X_test_OCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnlJXEbqZZGO"
      },
      "outputs": [],
      "source": [
        "def ensemble_results(cnn_model,x):\n",
        "    return cnn_model.predict(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqzkcPN2-Wr3"
      },
      "outputs": [],
      "source": [
        "#best load model with background image as input\n",
        "#best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_1/22_cnn.h5')#VGG19\n",
        "models_list = []\n",
        "best_model_fundus = keras.models.load_model('Modelos_CNN_Optuna/Teste_11/32_cnn.h5')#Dense169 - 224x224\n",
        "models_list.append(best_model_fundus)\n",
        "#load best model with ROI as input\n",
        "best_model_ROI = keras.models.load_model('Modelos_CNN_Optuna/Testes_50_Trials/Teste_2A/16_cnn.h5')#VGG19 - 128x128\n",
        "models_list.append(best_model_ROI)\n",
        "#load better model with OCT as input\n",
        "best_model_OCT = keras.models.load_model('Modelos_CNN_Optuna/Testes_30_Trials/Teste_3/11_cnn.h5')#Dense121 - 3D\n",
        "models_list.append(best_model_OCT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qE78lHDQZ44J"
      },
      "outputs": [],
      "source": [
        "list_predicted_prob = []\n",
        "for i in range (len(models_list)):\n",
        "  list_predicted_prob.append(ensemble_results(models_list[i], x_test_list[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRJRNcZdf4J"
      },
      "source": [
        "Average Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HluJZeOAdhjD"
      },
      "outputs": [],
      "source": [
        "# compute mean probabilities\n",
        "mean_probabilities = np.mean(list_predicted_prob, axis=0)\n",
        "\n",
        "# compute predicted class\n",
        "# argmax uses 1st ocurrance in case of a tie\n",
        "y_predicted_class = np.argmax(mean_probabilities, axis=1)\n",
        "#gera_csv\n",
        "gera_csv(y_predicted_class, classification_results_dir,str(trial_number))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mode ensemble"
      ],
      "metadata": {
        "id": "PGoTSwDs6CW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## function to identify cases where 2 models predicted in a class\n",
        "def conta_votos_2x1(v1,v2,v3):\n",
        "  votos = []\n",
        "  # 1, 2 ou 3\n",
        "  index1 = v1.index('1')#model fundus stage predicition\n",
        "  votos.append(index1)\n",
        "  index2 = v2.index('1')#model roi stage predicition\n",
        "  votos.append(index2)\n",
        "  index3 = v3.index('1')#model oct stage predicition\n",
        "  votos.append(index3)\n",
        "\n",
        "  #verifica quem teve 2 votos\n",
        "  if votos.count(1) == 2:\n",
        "    a=[v1[0],'1','0','0']\n",
        "    return (a)\n",
        "  if votos.count(2) == 2:\n",
        "    a=[v1[0],'0','1','0']\n",
        "    return (a)\n",
        "  if votos.count(3) == 2:\n",
        "    a=[v1[0],'0','0','1']\n",
        "    return (a)"
      ],
      "metadata": {
        "id": "9NySfjkgzWaB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function gives the following priority to resolve conflicts\n",
        "# if the oct models gives an initial diagnosis result, it will be kept\n",
        "# if the fundus model is advanced or normal it will be kept\n",
        "# otherwise the fundus model result will be kept\n",
        "def analisa_conflito(v1,v2,v3):\n",
        "  votos = []\n",
        "  index1 = v1.index('1')\n",
        "  votos.append(index1)\n",
        "  index2 = v2.index('1')\n",
        "  votos.append(index2)\n",
        "  index3 = v3.index('1')\n",
        "  votos.append(index3)\n",
        "\n",
        "  if index3 == 2: #result of oct model is early glaucoma\n",
        "    return ([v1[0],'0','1','0'])\n",
        "  else:\n",
        "    return v1"
      ],
      "metadata": {
        "id": "YSeQ_dWm0Wpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pprint\n",
        "\n",
        "#extract the data from the csv file and put it in a list\n",
        "fundus_predictions = []\n",
        "roi_predictions = []\n",
        "oct_predictions = []\n",
        "#ensemble = []\n",
        "#csv of best models\n",
        "results_fundo = os.path.join(path_dataset, 'Melhores resultados/Classification_Results_3_Trial_32.csv')#classification of fundus model\n",
        "results_roi = os.path.join(path_dataset, 'Melhores resultados/Classification_Results_3_Trial_16.csv')#classification of roi model\n",
        "results_oct = os.path.join(path_dataset, 'Melhores resultados/Classification_Results_2_Trial_11.csv')#classification of oct model\n",
        "\n",
        "with open(results_fundo,'r') as f:\n",
        "    lines=csv.reader(f)\n",
        "    for key, line in enumerate(lines):\n",
        "        fundus_predictions.append(line)\n",
        "\n",
        "with open(results_roi,'r') as f:\n",
        "    lines=csv.reader(f)\n",
        "    for key, line in enumerate(lines):\n",
        "        roi_predictions.append(line)\n",
        "\n",
        "with open(results_oct,'r') as f:\n",
        "    lines=csv.reader(f)\n",
        "    for key, line in enumerate(lines):\n",
        "        oct_predictions.append(line)"
      ],
      "metadata": {
        "id": "3ST3H70STHRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble = []\n",
        "for i in range(len(fundus_predictions)):\n",
        "  if i == 0:\n",
        "    ensemble.append(fundus_predictions[i])#Name of each column\n",
        "  elif fundus_predictions[i] == roi_predictions[i] and roi_predictions[i] == oct_predictions[i]:#All models predict same class\n",
        "    ensemble.append(fundus_predictions[i])\n",
        "  elif fundus_predictions[i] != roi_predictions[i] and roi_predictions[i] != oct_predictions[i] and fundus_predictions[i] != oct_predictions[i]:#each model predicts a different class\n",
        "    ensemble.append(fundus_predictions[i])#keeps fundus model result\n",
        "    #ensemble.append(analisa_conflito(fundus_predictions[i], roi_predictions[i], oct_predictions[i]))#uses function to determine final result\n",
        "  else:\n",
        "    ensemble.append(conta_votos_2x1(fundus_predictions[i], roi_predictions[i], oct_predictions[i]))#checks cases where 2 models predicted the same class\n",
        "\n",
        "#create a csv\n",
        "results_file_path = os.path.join(val_path_dataset, \"./Classification Results Optuna/Ensemble/Classification_Results.csv\")\n",
        "with open(results_file_path, 'w', encoding='UTF8', newline='') as f:\n",
        "  writer = csv.writer(f)\n",
        "  #writer.writerow(header)\n",
        "  writer.writerows(ensemble)"
      ],
      "metadata": {
        "id": "Tv0-Fviu0aqE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4odujajzijHe",
        "qPN2YsxZmki0",
        "dP4xlwCWNFfn",
        "wKUezmqObFkP",
        "OjM_QudD7nP9",
        "uA0tFXlBC2hF",
        "QFkaOH2XWw69",
        "W4R9n8EEXRcq",
        "sC_a8OAJu7We",
        "9fmQ-6Rv2qTD",
        "C0HE_wEfLkWD",
        "gmuOfFOAf03E",
        "9PoS0_oRjGRZ",
        "BK4BQEDOMIyk"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}